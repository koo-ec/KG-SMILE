{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "uWz0SMuX5FFp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uWz0SMuX5FFp",
    "outputId": "bf5567d7-1482-48bf-84a0-76535e79a1f5"
   },
   "outputs": [],
   "source": [
    "# Install the OpenAI and LangChain libraries\n",
    "# - `openai`: Provides access to OpenAI's GPT models for tasks like text generation, embeddings, and completions.\n",
    "# - `langchain`: A framework for building applications using large language models (LLMs).\n",
    "#                Includes tools for chaining prompts, memory, and integrations like knowledge graphs.\n",
    "!pip install -q openai langchain\n",
    "# Attempt to install the LangChain Community library\n",
    "# - `langchain-community`: This may refer to a community-supported version or extensions of LangChain.\n",
    "#   Ensure this package exists and is maintained if errors occur during installation.\n",
    "!pip install -q langchain-community"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa45a12-bf97-4b57-85ef-b8bd04ce5d9c",
   "metadata": {
    "id": "5fa45a12-bf97-4b57-85ef-b8bd04ce5d9c"
   },
   "source": [
    "This script initializes the OpenAI API client and defines a function to interact with the GPT model. The get_chat_response function sends a user-provided text input to the GPT model (gpt-3.5-turbo) and returns the model's response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "v2VIfINowvpp",
   "metadata": {
    "id": "v2VIfINowvpp"
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# Set the API key in the environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-MNL1gYbV6CyXkh2rwPxao_D7n8nSxwW4_0wozr5sUtT3BlbkFJoEpwVXUH_Z3deg71NI-mM8QqSOkOGzQ5WDXmQ8FQEA\" # Replace with your actual API key\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def get_chat_response(text):\n",
    "    \"\"\"\n",
    "    This function takes a text input and returns the chat completion message.\n",
    "    \"\"\"\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": text,\n",
    "            }\n",
    "        ],\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d292c1a5-9e7d-4231-b908-8f3950b8c259",
   "metadata": {
    "id": "d292c1a5-9e7d-4231-b908-8f3950b8c259"
   },
   "outputs": [],
   "source": [
    "import networkx as nx  # For creating and analyzing graphs/networks.\n",
    "\n",
    "import matplotlib.pyplot as plt  # For data visualization and plotting.\n",
    "\n",
    "import numpy as np  # For numerical operations and array handling.\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import random  # For generating random numbers.\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "from langchain.graphs.networkx_graph import NetworkxEntityGraph, KnowledgeTriple # Represents (subject, predicate, object) triples.\n",
    "\n",
    "from scipy.spatial.distance import cosine  # For cosine similarity/distance between vectors.\n",
    "\n",
    "from scipy.stats import wasserstein_distance  # For Wasserstein distance (probability distribution comparison).\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, BayesianRidge  # Regression models.\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups  # Fetch the 20 Newsgroups text dataset.\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score  # Model performance metrics.\n",
    "\n",
    "import matplotlib.colors as mcolors  # For handling and customizing colors in visualizations.\n",
    "\n",
    "import sklearn.metrics  # For evaluation metrics like accuracy, precision, recall, etc.\n",
    "\n",
    "import matplotlib.colors as mcolors  # For handling color schemes in plots\n",
    "\n",
    "import textwrap  # For wrapping text into fixed-width lines\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "from langchain.chains import GraphQAChain  # For question answering over knowledge graphs.\n",
    "\n",
    "# Prompt Engineering\n",
    "from langchain.prompts import PromptTemplate  # To define templates for LLM prompts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tmp-lg-yHhMc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tmp-lg-yHhMc",
    "outputId": "68a760c5-eea2-4fd5-dc08-504ea9ab09db"
   },
   "outputs": [],
   "source": [
    "!pip install  rdflib  SPARQLWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38451a30-0f18-44af-acd0-e085ab5ed48a",
   "metadata": {
    "id": "38451a30-0f18-44af-acd0-e085ab5ed48a"
   },
   "source": [
    "This script defines a knowledge graph using a set of triples representing entities (nodes) and their relationships (edges). The triples are categorized into parts based on themes, such as LLMs in the legal context, RAG integration, collaborations, and key people involved. The knowledge graph is constructed programmatically by adding these triples into the graph index, which allows for efficient querying and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1R_DL-vTgv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aa1R_DL-vTgv",
    "outputId": "0e678c1d-1792-4d7e-9969-9ea4d77e1982"
   },
   "outputs": [],
   "source": [
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "# Set up the DBpedia SPARQL endpoint\n",
    "sparql = SPARQLWrapper(\"https://dbpedia.org/sparql\")\n",
    "\n",
    "# SPARQL Query: Retrieve cybersecurity-related concepts and their triples\n",
    "query = \"\"\"\n",
    "PREFIX dbo: <http://dbpedia.org/ontology/>\n",
    "PREFIX dbr: <http://dbpedia.org/resource/>\n",
    "PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "\n",
    "SELECT ?concept ?label ?abstract\n",
    "WHERE {\n",
    "  ?concept rdf:type dbo:Software .\n",
    "  ?concept rdfs:label ?label .\n",
    "  ?concept dbo:abstract ?abstract .\n",
    "  FILTER (LANG(?label) = 'en' && LANG(?abstract) = 'en')\n",
    "  FILTER (CONTAINS(LCASE(?label), \"cyber\") || CONTAINS(LCASE(?label), \"security\") || CONTAINS(LCASE(?label), \"malware\"))\n",
    "}\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "sparql.setQuery(query)\n",
    "sparql.setReturnFormat(JSON)\n",
    "results = sparql.query().convert()\n",
    "\n",
    "# Initialize Knowledge Graph (KG) and portion tracking\n",
    "kg = []\n",
    "portion_indices = {}\n",
    "portion_counter = 1  # Start portion numbering\n",
    "triple_index = 0  # Track overall index\n",
    "\n",
    "print(\"\\nStructured Knowledge Graph:\\n\")\n",
    "\n",
    "for result in results[\"results\"][\"bindings\"]:\n",
    "    concept = result[\"concept\"][\"value\"].split(\"/\")[-1]  # Extracts entity name\n",
    "    label = result[\"label\"][\"value\"]\n",
    "    abstract = result[\"abstract\"][\"value\"]  # Store full abstract without truncation\n",
    "\n",
    "    # Store portion index range\n",
    "    start_index = triple_index\n",
    "    portion_indices[f\"Part {portion_counter}\"] = range(start_index, start_index + 3)  # Each part has 3 triples\n",
    "\n",
    "    # Print structured output\n",
    "    print(f\"\\n# Part {portion_counter}\")\n",
    "    print(f\"({concept}) → (type) → (Software)\")\n",
    "    print(f\"({concept}) → (label) → ({label})\")\n",
    "    print(f\"({concept}) → (abstract) →\")\n",
    "    print(abstract)  # Print full abstract with line breaks\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    # Store in KG\n",
    "    kg.append((concept, \"type\", \"Software\"))\n",
    "    kg.append((concept, \"label\", label))\n",
    "    kg.append((concept, \"abstract\", abstract))  # Store full abstract\n",
    "\n",
    "    # Increment indices\n",
    "    triple_index += 3\n",
    "    portion_counter += 1\n",
    "\n",
    "# Print portion indices separately\n",
    "print(\"\\nPortion Indices:\\n\")\n",
    "for part, index_range in portion_indices.items():\n",
    "    print(f\"{part}: {index_range}\")\n",
    "\n",
    "# Save KG to a text file\n",
    "with open(\"knowledge_graph_output.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for triple in kg:\n",
    "        f.write(f\"( {triple[0]} , {triple[1]} , {triple[2]})\\n\\n\")  # Ensuring full visibility\n",
    "\n",
    "print(\"\\nFinal Knowledge Graph saved as 'knowledge_graph_output.txt'.\")\n",
    "\n",
    "# Print the final KG in a readable format\n",
    "print(\"\\nFinal Knowledge Graph List:\\n\")\n",
    "for triple in kg:\n",
    "  print(\"(\", triple[0],\",\", triple[1],\", \",triple[2], \")\")  # Print without truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oh4wABT8q8UP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oh4wABT8q8UP",
    "outputId": "16c36cd2-ee2d-43bd-a4e1-7844dacfce5a"
   },
   "outputs": [],
   "source": [
    "print(\"Original KG node count:\", len(set(node for triple in kg for node in (triple[0], triple[2]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efG9suFIBXH",
   "metadata": {
    "id": "4efG9suFIBXH"
   },
   "outputs": [],
   "source": [
    "part_indices ={\n",
    "\"Part 1\": range(0, 3),\n",
    "\"Part 2\": range(3, 6),\n",
    "\"Part 3\": range(6, 9),\n",
    "\"Part 4\": range(9, 12),\n",
    "\"Part 5\": range(12, 15),\n",
    "\"Part 6\": range(15, 18),\n",
    "\"Part 7\": range(18, 21),\n",
    "\"Part 8\": range(21, 24),\n",
    "\"Part 9\": range(24, 27),\n",
    "\"Part 10\": range(27, 30)\n",
    "}\n",
    "part_names = list(part_indices.keys())\n",
    "\n",
    "# Instantiate the graph\n",
    "graph = NetworkxEntityGraph()\n",
    "\n",
    "# Build the graph from the knowledge triples\n",
    "for (node1, relation, node2) in kg:\n",
    "    graph.add_triple(KnowledgeTriple(node1, relation, node2))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "818a9197-5541-4b89-ba45-330c0da504a5",
   "metadata": {
    "id": "818a9197-5541-4b89-ba45-330c0da504a5"
   },
   "source": [
    " two different methods for working with knowledge graphs (KGs)\n",
    "\n",
    "| Feature                   | NetworkX (`nx.DiGraph`)                        | Knowledge Graph Framework (`add_triple`)      |\n",
    "|---------------------------|-----------------------------------------------|----------------------------------------------|\n",
    "| **Primary Purpose**       | General-purpose graph operations.             | Semantic knowledge representation and reasoning. |\n",
    "| **Edge Representation**   | Relation stored as edge attribute (`label`).  | Explicitly stores as a `KnowledgeTriple`.    |\n",
    "| **Scalability**           | Efficient for smaller graphs.                 | Often optimized for large-scale KGs.         |\n",
    "| **Functionality**         | Basic graph analysis (e.g., paths, cycles).   | Advanced reasoning, semantic queries, or RAG.|\n",
    "| **Ease of Use**           | Simple and intuitive.                         | May require setup and specific tooling.      |\n",
    "\n",
    "---\n",
    "\n",
    "Which to Use?\n",
    "\n",
    "- Use **NetworkX** if you:\n",
    "  - Need quick, general-purpose graph creation and manipulation.\n",
    "  - Plan to perform basic analysis or visualization.\n",
    "\n",
    "- Use a **dedicated KG framework** if you:\n",
    "  - Are working on semantic reasoning, ontology-based querying, or RAG.\n",
    "  - Need advanced features like integration with NLP pipelines or search engines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a265c98e-4c73-4429-bf6e-0dab5e3a566b",
   "metadata": {
    "id": "a265c98e-4c73-4429-bf6e-0dab5e3a566b"
   },
   "source": [
    "Visualizes the knowledge graph as a directed graph using NetworkX and Matplotlib. Nodes represent entities, and edges depict relationships with labels for clarity. The layout uses spring positioning with increased spacing for readability. Custom node colors and labeled edges enhance the visualization, displayed without axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "C-KMAYlwYZBV",
   "metadata": {
    "id": "C-KMAYlwYZBV"
   },
   "outputs": [],
   "source": [
    "def wrap_text(text, max_words=8):\n",
    "    \"\"\"Wrap text if it contains more than `max_words` words.\"\"\"\n",
    "    words = text.split()\n",
    "    return \"Explanation\" if len(words) > max_words else text\n",
    "\n",
    "def visualize_graph_with_chains(kg, part_indices):\n",
    "    \"\"\"\n",
    "    Visualize a directed graph highlighting nodes and edges by chain membership.\n",
    "\n",
    "    Parameters:\n",
    "        kg (list of tuples): The knowledge graph as a list of (node1, relation, node2).\n",
    "        part_indices (dict): A dictionary where keys are chain names and values are lists of indices\n",
    "                             corresponding to the `kg` entries in each chain.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Create graph\n",
    "    G = nx.DiGraph()\n",
    "    for node1, relation, node2 in kg:\n",
    "        G.add_edge(node1, node2, label=relation)\n",
    "\n",
    "    # Generate positions for the graph\n",
    "    pos = nx.spring_layout(G, k=8, iterations=100, seed=0)\n",
    "\n",
    "    # Define color maps\n",
    "    chain_cmap = mcolors.LinearSegmentedColormap.from_list('chain_colors', ['#66c2a5', '#fc8d62', '#8da0cb', '#e78ac3', '#a6d854'])\n",
    "    chain_norm = mcolors.Normalize(vmin=0, vmax=len(part_indices) - 1)\n",
    "\n",
    "    # Initialize node and edge colors\n",
    "    node_colors = ['lightblue'] * len(G.nodes())\n",
    "    edge_colors = ['gray'] * len(G.edges())\n",
    "\n",
    "    # Step 1: Assign distinct colors for each chain\n",
    "    chain_color_map = {}\n",
    "    for i, (chain_name, indices) in enumerate(part_indices.items()):\n",
    "        color = chain_cmap(chain_norm(i))\n",
    "        chain_color_map[chain_name] = color\n",
    "\n",
    "    # Step 2: Color nodes based on the chain they belong to\n",
    "    node_chain_map = {}\n",
    "    for chain_name, indices in part_indices.items():\n",
    "        color = chain_color_map[chain_name]\n",
    "        for idx in indices:\n",
    "            node1, relation, node2 = kg[idx]\n",
    "\n",
    "            # Update node colors based on chain\n",
    "            if node1 in G.nodes:\n",
    "                node_chain_map[node1] = chain_name\n",
    "                node_colors[list(G.nodes).index(node1)] = color\n",
    "            if node2 in G.nodes:\n",
    "                node_chain_map[node2] = chain_name\n",
    "                node_colors[list(G.nodes).index(node2)] = color\n",
    "\n",
    "    # Step 3: Assign edge colors based on the chain\n",
    "    for i, (node1, node2) in enumerate(G.edges()):\n",
    "        for chain_name, indices in part_indices.items():\n",
    "            color = chain_color_map[chain_name]\n",
    "            for idx in indices:\n",
    "                n1, _, n2 = kg[idx]\n",
    "                if (node1, node2) == (n1, n2):\n",
    "                    edge_colors[i] = color\n",
    "                    break\n",
    "\n",
    "    # Apply label filtering\n",
    "    wrapped_labels = {node: wrap_text(node) for node in G.nodes()}\n",
    "\n",
    "    # Create the figure with subplots\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(20, 8), dpi=600)\n",
    "\n",
    "    # Left: Original Knowledge Graph\n",
    "    nx.draw_networkx_nodes(G, pos, node_color='lightblue', node_size=1200, ax=axs[0])\n",
    "    nx.draw_networkx_edges(G, pos, edge_color='gray', width=1.2, ax=axs[0])\n",
    "    nx.draw_networkx_labels(G, pos, labels=wrapped_labels, font_size=6, ax=axs[0])\n",
    "    edge_labels = nx.get_edge_attributes(G, 'label')\n",
    "    wrapped_edge_labels = {edge: wrap_text(label) for edge, label in edge_labels.items()}\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=wrapped_edge_labels, font_size=6, ax=axs[0])\n",
    "    axs[0].set_title(\"Original Knowledge Graph\", fontsize=10)\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    # Right: Highlighted Nodes Based on Chains\n",
    "    nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=1200, ax=axs[1], edgecolors='black')\n",
    "    nx.draw_networkx_edges(G, pos, edge_color=edge_colors, width=1.5, ax=axs[1])\n",
    "    nx.draw_networkx_labels(G, pos, labels=wrapped_labels, font_size=6, ax=axs[1])\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=wrapped_edge_labels, font_size=6, ax=axs[1])\n",
    "    axs[1].set_title(\"Graph Highlighted by Chain Membership\", fontsize=10)\n",
    "    axs[1].axis('off')\n",
    "\n",
    "    # Create a legend for chain colors\n",
    "    handles = [plt.Line2D([0], [0], marker='o', color=color, markersize=10, linestyle='', label=chain_name)\n",
    "               for chain_name, color in chain_color_map.items()]\n",
    "    axs[1].legend(handles=handles, title=\"Chains\", loc='upper right', fontsize=8)\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "    # Print which nodes belong to which chain\n",
    "    print(\"\\n--- Node Chain Mapping ---\")\n",
    "    for node, chain in node_chain_map.items():\n",
    "        print(f\"Node '{node}' belongs to chain '{chain}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZEIk-T9uYg9r",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ZEIk-T9uYg9r",
    "outputId": "96c42b8f-c404-4163-c4ef-bfc3157ce2ee"
   },
   "outputs": [],
   "source": [
    "visualize_graph_with_chains(kg, part_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d728dc4-f242-451f-94db-b1a5fcd14243",
   "metadata": {
    "id": "2d728dc4-f242-451f-94db-b1a5fcd14243"
   },
   "source": [
    "Defines a function to perturb the knowledge graph by selectively removing triples belonging to specified parts. This allows testing the impact of missing information on downstream tasks or analysis. The function filters out triples associated with the indices of the parts to be removed and returns the modified knowledge graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "A39WF0Pa_OYY",
   "metadata": {
    "id": "A39WF0Pa_OYY"
   },
   "outputs": [],
   "source": [
    "def perturb_kg_by_removing_parts(kg, parts_to_remove):\n",
    "    \"\"\"\n",
    "    Perturbs the knowledge graph by removing triples from the specified parts.\n",
    "\n",
    "    Parameters:\n",
    "    - kg: The full knowledge graph triples list\n",
    "    - parts_to_remove: List of part names to remove\n",
    "\n",
    "    Returns:\n",
    "    - perturbed_kg: The perturbed KG without the specified parts\n",
    "    \"\"\"\n",
    "    perturbed_kg = []\n",
    "\n",
    "    # Collect indices of the triples to keep based on parts to remove\n",
    "    indices_to_remove = set()\n",
    "    for part in parts_to_remove:\n",
    "        indices_to_remove.update(part_indices[part])\n",
    "\n",
    "    # Add triples that are not in the indices to remove\n",
    "    perturbed_kg = [triple for i, triple in enumerate(kg) if i not in indices_to_remove]\n",
    "\n",
    "    return perturbed_kg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d447e4-a68c-493e-9d49-b3a99a97dc19",
   "metadata": {
    "id": "69d447e4-a68c-493e-9d49-b3a99a97dc19"
   },
   "source": [
    "This function computes the embedding for a given text using a specified model. It processes the text by removing newline characters and queries the OpenAI embeddings API to generate a vector representation, useful for similarity comparisons and downstream tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TLLTyp-y_UFV",
   "metadata": {
    "id": "TLLTyp-y_UFV"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import unicodedata\n",
    "import re\n",
    "embedding_cache = {}\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    Normalize text by removing excessive spaces, normalizing Unicode characters,\n",
    "    and converting to lowercase.\n",
    "    \"\"\"\n",
    "    text = text.replace(\"\\n\", \" \").strip()  # Remove newlines and extra spaces\n",
    "    text = unicodedata.normalize(\"NFKC\", text)  # Normalize Unicode characters\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Replace multiple spaces with a single space\n",
    "    text = text.lower()  # Convert to lowercase (optional but recommended)\n",
    "    return text\n",
    "\n",
    "def get_embedding(text):\n",
    "    text = normalize_text(text)\n",
    "    if text in embedding_cache:\n",
    "        return embedding_cache[text]  # Return cached embedding\n",
    "    embedding = client.embeddings.create(input=[text], model=EMBEDDING_MODEL).data[0].embedding\n",
    "    embedding_cache[text] = embedding  # Store result in cache\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2993aae2-97cc-4198-bdcb-7f950a011929",
   "metadata": {
    "id": "2993aae2-97cc-4198-bdcb-7f950a011929"
   },
   "source": [
    "Defines a function to query a GraphQAChain with a question and temperature setting, returning the answer and its embedding. The function initializes the chain with a specified graph and temperature, processes the question, and computes the embedding for the returned answer, facilitating downstream analysis or comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2286902d-b91b-41d1-ae21-c450a91a5c86",
   "metadata": {
    "id": "2286902d-b91b-41d1-ae21-c450a91a5c86"
   },
   "outputs": [],
   "source": [
    "def get_answer_and_embedding(question: str, temp: float, graph):\n",
    "    \"\"\"\n",
    "    Sends a question and temperature to the GraphQAChain and returns the original answer string\n",
    "    and its embedding as separate outputs.\n",
    "\n",
    "    Args:\n",
    "        question (str): The question to ask the chain.\n",
    "        temp (float): The temperature setting for the OpenAI model.\n",
    "        graph: The graph object for the GraphQAChain.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str, list]: The original answer as a string and its embedding as a list.\n",
    "    \"\"\"\n",
    "    # Initialize the GraphQAChain with the specified temperature\n",
    "    chain = GraphQAChain.from_llm(OpenAI(temperature=temp), graph=graph, verbose=False)\n",
    "\n",
    "    # Run the question through the chain to get the answer\n",
    "    original_answer = chain.run(question)\n",
    "    original_answer_str = str(original_answer)\n",
    "\n",
    "    # Compute the embedding for the original answer\n",
    "    original_answer_embedding = get_embedding(original_answer)\n",
    "\n",
    "    # Return both answer and embedding separately\n",
    "    return original_answer_str, original_answer_embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecce233-c67e-4748-b742-a66dd99f80b4",
   "metadata": {
    "id": "4ecce233-c67e-4748-b742-a66dd99f80b4"
   },
   "source": [
    "Defines the question to query the GraphQAChain or knowledge retrieval system. Here, the question \"What is RAG?\" seeks information about Retrieval-Augmented Generation, a framework that integrates external knowledge bases to improve the accuracy and reliability of AI-generated responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b16783-7c57-42d0-9bfb-ea298af379f0",
   "metadata": {
    "id": "c9b16783-7c57-42d0-9bfb-ea298af379f0"
   },
   "outputs": [],
   "source": [
    "question = \"what is Network_Security_Services?\"\n",
    "#Portion 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cffabbe-3a4e-420c-aeb9-0a13ddfe3306",
   "metadata": {
    "id": "5cffabbe-3a4e-420c-aeb9-0a13ddfe3306"
   },
   "source": [
    "This snippet sets the temperature parameter to 0 for deterministic response generation and queries the GraphQAChain with the question, \"What is RAG?\". The function get_answer_and_embedding returns the original answer as a string along with its embedding. The answer is then printed for review.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10979ac4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "10979ac4",
    "outputId": "6187b3c8-3d23-4c4d-f09a-7b640c0b93cb"
   },
   "outputs": [],
   "source": [
    "# Initialize the LLM\n",
    "llm = OpenAI(temperature=0)\n",
    "temp = 0\n",
    "original_answer_str, original_answer_embedding = get_answer_and_embedding(question, temp, graph)\n",
    "print(original_answer_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caab5a60-2b82-4f71-ba5d-b97972336298",
   "metadata": {
    "id": "caab5a60-2b82-4f71-ba5d-b97972336298"
   },
   "outputs": [],
   "source": [
    "# Define the original vector (all parts present)\n",
    "original = np.array([1, 1, 1, 1, 1,1, 1, 1, 1, 1])\n",
    "original = original.reshape(1, -1)  # Shape becomes (1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac9f22a-deed-47b7-a033-fd9773d7cb20",
   "metadata": {
    "id": "6ac9f22a-deed-47b7-a033-fd9773d7cb20"
   },
   "source": [
    "# Consistency: Providing similar explanations for similar queries or inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc5f227-afe0-4013-9d89-41385257d368",
   "metadata": {
    "id": "dbc5f227-afe0-4013-9d89-41385257d368"
   },
   "source": [
    "This function calculates feature importance coefficients by analyzing the impact of perturbations\n",
    "on a knowledge graph's response fidelity. It removes parts of the graph, generates perturbed responses,\n",
    "and measures their similarity to the original response using cosine similarity. A linear regression model\n",
    "is trained on the perturbation vectors and corresponding similarity values, with weights applied to account\n",
    "for the perturbations' relative importance. The resulting coefficients indicate the significance of each\n",
    "graph component in preserving response fidelity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0074ab26-71d3-4d95-8113-693ba1a9dfcf",
   "metadata": {
    "id": "0074ab26-71d3-4d95-8113-693ba1a9dfcf"
   },
   "outputs": [],
   "source": [
    "def calculate_coefficients(original, kg, part_names, question, original_answer_embedding, original_answer_str):\n",
    "    \"\"\"\n",
    "    Function to calculate coefficients for perturbations on a knowledge graph.\n",
    "    It removes parts of the KG, generates perturbed responses, and calculates coefficients.\n",
    "\n",
    "    Parameters:\n",
    "    - original: Original vector (numpy array)\n",
    "    - kg: Knowledge graph (list of triples)\n",
    "    - part_names: List of part names in the KG\n",
    "    - question: Question for GraphQAChain\n",
    "    - original_answer_embedding: Embedding of the original answer\n",
    "    - original_answer_str: Original answer text\n",
    "\n",
    "    Returns:\n",
    "    - coeff: Coefficients from linear regression\n",
    "    \"\"\"\n",
    "    similarities_wd = []\n",
    "    perturbations_vect2 = []\n",
    "    perturbation_texts = []\n",
    "    generated_embeddings = []\n",
    "    epsilon = 1e-6\n",
    "\n",
    "    for i in range(20):  # Number of perturbations\n",
    "        perturbation_vector = original.copy().flatten()\n",
    "        num_parts_to_remove = random.randint(1, len(part_names))\n",
    "        parts_to_remove_indices = random.sample(range(len(part_names)), num_parts_to_remove)\n",
    "\n",
    "        for part_idx in parts_to_remove_indices:\n",
    "            perturbation_vector[part_idx] = 0\n",
    "\n",
    "        perturbations_vect2.append(perturbation_vector)\n",
    "        parts_to_remove = [part_names[idx] for idx in parts_to_remove_indices]\n",
    "\n",
    "        # Call the perturb_kg_by_removing_parts function directly\n",
    "        perturbed_kg = perturb_kg_by_removing_parts(kg, parts_to_remove)\n",
    "\n",
    "        graph_temp = NetworkxEntityGraph()\n",
    "        for (node1, relation, node2) in perturbed_kg:\n",
    "            graph_temp.add_triple(KnowledgeTriple(node1, relation, node2))\n",
    "\n",
    "        chain = GraphQAChain.from_llm(OpenAI(temperature=0), graph=graph_temp, verbose=False)\n",
    "        temp_response = chain.run(question)\n",
    "\n",
    "        perturbation_texts.append(temp_response)\n",
    "\n",
    "        # Call the get_embedding function directly\n",
    "        temp_response_embedding = get_embedding(temp_response)\n",
    "        generated_embeddings.append(temp_response_embedding)\n",
    "\n",
    "        # Calculate Wasserstein distance\n",
    "        similarity_wd = wasserstein_distance(original_answer_embedding, temp_response_embedding)\n",
    "        similarities_wd.append(similarity_wd)\n",
    "\n",
    "    perturbations_vect2 = np.array(perturbations_vect2)\n",
    "    distances = sklearn.metrics.pairwise_distances(perturbations_vect2, original, metric='cosine').ravel()\n",
    "\n",
    "    kernel_width = 0.25\n",
    "    weights = np.sqrt(np.exp(-(distances**2) / kernel_width**2))\n",
    "\n",
    "    inverse_similarities_wd = [1.0 / (dist + epsilon) for dist in similarities_wd]\n",
    "\n",
    "    # Scale inverse Wasserstein distances\n",
    "    min_value = min(inverse_similarities_wd)\n",
    "    max_value = max(inverse_similarities_wd)\n",
    "    if min_value == max_value:\n",
    "      print(\"Warning: min_value and max_value are equal. Avoiding division by zero.\")\n",
    "      scaled_inverse_similarities_wd = [1.0 for _ in inverse_similarities_wd]  # Assign a constant\n",
    "    else:\n",
    "      scaled_inverse_similarities_wd = [\n",
    "          (value - min_value) / (max_value - min_value) for value in inverse_similarities_wd\n",
    "      ]\n",
    "\n",
    "    # Linear regression for cosine similarities\n",
    "    simpler_model = LinearRegression()\n",
    "    # Linear regression for scaled inverse Wasserstein distances\n",
    "    simpler_model.fit(X=perturbations_vect2, y=scaled_inverse_similarities_wd, sample_weight=weights)\n",
    "    coeff = simpler_model.coef_\n",
    "    return coeff\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b6577d-d5e4-4325-b310-ea322ae90ab3",
   "metadata": {
    "id": "e9b6577d-d5e4-4325-b310-ea322ae90ab3"
   },
   "source": [
    "This script runs the `calculate_coefficients` function 50 times, each time perturbing the knowledge graph\n",
    "and computing feature importance coefficients. The iteration number (test case) is prepended to the coefficients\n",
    "from each run, and the results are stored in a DataFrame with appropriate column headers. The DataFrame is then\n",
    "exported to a CSV file named \"Book1.csv\" for further analysis or reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ef813c-3fdf-4369-b4f4-34f9472cfd57",
   "metadata": {
    "id": "60ef813c-3fdf-4369-b4f4-34f9472cfd57"
   },
   "outputs": [],
   "source": [
    "coefficients_list = []\n",
    "\n",
    "# Run the function 50 times and collect coefficients\n",
    "for i in range(1, 51):  # Run 50 iterations\n",
    "    coeff = calculate_coefficients(\n",
    "        original=original,\n",
    "        kg=kg,\n",
    "        part_names=part_names,\n",
    "        question=question,\n",
    "        original_answer_embedding=original_answer_embedding,\n",
    "        original_answer_str=original_answer_str,\n",
    "    )\n",
    "    # Prepend the iteration number (Test Case) to the coefficients\n",
    "    coefficients_list.append([i] + coeff.tolist())\n",
    "\n",
    "# Create the DataFrame with proper headers\n",
    "column_names = ['Test Case'] + [f'part {i + 1}' for i in range(len(coeff))]\n",
    "coefficients_df = pd.DataFrame(coefficients_list, columns=column_names)\n",
    "\n",
    "# Save to CSV\n",
    "coefficients_df.to_csv(\"Book1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6bbea4-e723-4c6c-be43-edd532893fce",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bc6bbea4-e723-4c6c-be43-edd532893fce",
    "outputId": "00ef2e83-2202-4656-95a5-86386b37969b"
   },
   "outputs": [],
   "source": [
    "# Load the file into a DataFrame\n",
    "df = pd.read_csv('Book1.csv')\n",
    "\n",
    "# Display the columns of the DataFrame\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c96875-9b06-409f-997e-4d48d87817ec",
   "metadata": {
    "id": "a7c96875-9b06-409f-997e-4d48d87817ec"
   },
   "source": [
    "This script visualizes the coefficients for different parts of a knowledge graph across multiple test cases\n",
    "using a half-violin plot combined with a scatter plot (\"rain\" effect). The violin plot shows the distribution\n",
    "of values for each part, while the scatter plot adds individual test case points with slight jitter for clarity.\n",
    "Unique colors are assigned to each test case, and the plot is customized with a title, axis labels, and a legend\n",
    "to improve interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5RMJJrHTp3Ia",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 494
    },
    "id": "5RMJJrHTp3Ia",
    "outputId": "f4c960df-02a2-4370-ef60-bd707379f7ce"
   },
   "outputs": [],
   "source": [
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(\"Book1.csv\")\n",
    "\n",
    "# Define the 10 parts explicitly to ensure they are included\n",
    "parts = ['part 1', 'part 2', 'part 3', 'part 4', 'part 5',\n",
    "         'part 6', 'part 7', 'part 8', 'part 9', 'part 10']\n",
    "\n",
    "# Reshape the data for seaborn (long format)\n",
    "df_melted = df.melt(id_vars=['Test Case'], value_vars=parts, var_name='Part', value_name='Values')\n",
    "\n",
    "# Set figure size and style\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Create a simple box plot for 10 parts\n",
    "sns.boxplot(\n",
    "    data=df_melted, x=\"Part\", y=\"Values\", palette=\"Set2\"\n",
    ")\n",
    "\n",
    "# Customize the plot\n",
    "plt.title(\"Box Plot for 10 Parts\", fontsize=14)\n",
    "plt.ylabel(\"Values\", fontsize=12)\n",
    "plt.xlabel(\"Part\", fontsize=12)\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for clarity\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49eb09cb-6e9b-4056-90b0-19272a45a542",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "eb1301f0-4ef1-4da4-af3b-a12298d0b3d8"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
