{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uWz0SMuX5FFp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uWz0SMuX5FFp",
    "outputId": "cb288311-0499-4611-d137-c18f94ebb907"
   },
   "outputs": [],
   "source": [
    "# Install the OpenAI and LangChain libraries\n",
    "# - `openai`: Provides access to OpenAI's GPT models for tasks like text generation, embeddings, and completions.\n",
    "# - `langchain`: A framework for building applications using large language models (LLMs).\n",
    "#                Includes tools for chaining prompts, memory, and integrations like knowledge graphs.\n",
    "\n",
    "!pip install -q openai langchain\n",
    "\n",
    "\n",
    "# Attempt to install the LangChain Community library\n",
    "# - `langchain-community`: This may refer to a community-supported version or extensions of LangChain.\n",
    "#   Ensure this package exists and is maintained if errors occur during installation.\n",
    "\n",
    "!pip install -q langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "TQwaa-8771BB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TQwaa-8771BB",
    "outputId": "07450458-a3e5-4feb-c66d-1b6ed2a08236"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rdflib\n",
      "  Downloading rdflib-7.1.3-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting SPARQLWrapper\n",
      "  Downloading SPARQLWrapper-2.0.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\hp victus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.8.3)\n",
      "Requirement already satisfied: pyparsing<4,>=2.1.0 in c:\\users\\hp victus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rdflib) (3.1.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\hp victus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\hp victus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\hp victus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (4.50.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\hp victus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in c:\\users\\hp victus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp victus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (24.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\hp victus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\hp victus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp victus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading rdflib-7.1.3-py3-none-any.whl (564 kB)\n",
      "   ---------------------------------------- 0.0/564.9 kB ? eta -:--:--\n",
      "    --------------------------------------- 10.2/564.9 kB ? eta -:--:--\n",
      "   ------ --------------------------------- 92.2/564.9 kB 1.3 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 225.3/564.9 kB 1.7 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 368.6/564.9 kB 2.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 501.8/564.9 kB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 564.9/564.9 kB 2.4 MB/s eta 0:00:00\n",
      "Downloading SPARQLWrapper-2.0.0-py3-none-any.whl (28 kB)\n",
      "Installing collected packages: rdflib, SPARQLWrapper\n",
      "Successfully installed SPARQLWrapper-2.0.0 rdflib-7.1.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: C:\\Users\\HP Victus\\AppData\\Local\\Programs\\Python\\Python312\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install  rdflib  SPARQLWrapper  matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94378d7b-2345-4f9e-ab63-8b4134b69484",
   "metadata": {
    "id": "94378d7b-2345-4f9e-ab63-8b4134b69484"
   },
   "source": [
    "\n",
    "This script initializes the OpenAI API client and defines a function to interact with the GPT model.\n",
    "The `get_chat_response` function sends a user-provided text input to the GPT model (gpt-3.5-turbo)\n",
    "and returns the model's response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TxGg4AFMt-MD",
   "metadata": {
    "id": "TxGg4AFMt-MD"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# Set the API key in the environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-MNL1gYbV6CyXkh2rwPxao_D7n8nSxwW4_0wozr5sUtT3BlbkFJoEpwVXUH_Z3deg71NI-mM8QqSOkOGzQ5WDXmQ8FQEA\" # Replace with your actual API key\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def get_chat_response(text):\n",
    "    \"\"\"\n",
    "    This function takes a text input and returns the chat completion message.\n",
    "    \"\"\"\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": text,\n",
    "            }\n",
    "        ],\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d292c1a5-9e7d-4231-b908-8f3950b8c259",
   "metadata": {
    "id": "d292c1a5-9e7d-4231-b908-8f3950b8c259"
   },
   "outputs": [],
   "source": [
    "import networkx as nx  # For creating and analyzing graphs/networks.\n",
    "\n",
    "import matplotlib.pyplot as plt  # For data visualization and plotting.\n",
    "\n",
    "import numpy as np  # For numerical operations and array handling.\n",
    "\n",
    "import random  # For generating random numbers.\n",
    "\n",
    "from langchain.graphs.networkx_graph import NetworkxEntityGraph, KnowledgeTriple # Represents (subject, predicate, object) triples.\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "from scipy.spatial.distance import cosine  # For cosine similarity/distance between vectors.\n",
    "\n",
    "from scipy.stats import wasserstein_distance  # For Wasserstein distance (probability distribution comparison).\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, BayesianRidge  # Regression models.\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups  # Fetch the 20 Newsgroups text dataset.\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score  # Model performance metrics.\n",
    "\n",
    "import matplotlib.colors as mcolors  # For handling and customizing colors in visualizations.\n",
    "\n",
    "import sklearn.metrics  # For evaluation metrics like accuracy, precision, recall, etc.\n",
    "\n",
    "import matplotlib.colors as mcolors  # For handling color schemes in plots\n",
    "\n",
    "import textwrap  # For wrapping text into fixed-width lines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9B6t08byg-aL",
   "metadata": {
    "id": "9B6t08byg-aL"
   },
   "source": [
    "If you're using OpenAI with a GraphIndexCreator, and it’s unavailable in the new LangChain version, you can adapt the code using NetworkxEntityGraph for creating and querying a graph with an LLM, or we replaced it with a custom CustomGraphIndexCreator that integrates NetworkxEntityGraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cd037f-dfb0-43f1-9bfd-e352ab79d3eb",
   "metadata": {
    "id": "18cd037f-dfb0-43f1-9bfd-e352ab79d3eb"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import GraphQAChain  # For question answering over knowledge graphs.\n",
    "\n",
    "# Prompt Engineering\n",
    "from langchain.prompts import PromptTemplate  # To define templates for LLM prompts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e21c359-c53b-4217-9563-26ce499f2cd2",
   "metadata": {
    "id": "8e21c359-c53b-4217-9563-26ce499f2cd2"
   },
   "source": [
    "\n",
    "This script defines a knowledge graph using a set of triples representing entities (nodes)\n",
    "and their relationships (edges). The triples are categorized into parts based on themes,\n",
    "such as LLMs in the legal context, RAG integration, collaborations, and key people involved.\n",
    "The knowledge graph is constructed programmatically by adding these triples into the graph\n",
    "index, which allows for efficient querying and analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aK0K9pECaDq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8aK0K9pECaDq",
    "outputId": "4cb783f9-a94e-4d88-dcde-5c779a6a3879"
   },
   "outputs": [],
   "source": [
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "# Set up the DBpedia SPARQL endpoint\n",
    "sparql = SPARQLWrapper(\"https://dbpedia.org/sparql\")\n",
    "\n",
    "# SPARQL Query: Retrieve cybersecurity-related concepts and their triples\n",
    "query = \"\"\"\n",
    "PREFIX dbo: <http://dbpedia.org/ontology/>\n",
    "PREFIX dbr: <http://dbpedia.org/resource/>\n",
    "PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "\n",
    "SELECT ?concept ?label ?abstract\n",
    "WHERE {\n",
    "  ?concept rdf:type dbo:Software .\n",
    "  ?concept rdfs:label ?label .\n",
    "  ?concept dbo:abstract ?abstract .\n",
    "  FILTER (LANG(?label) = 'en' && LANG(?abstract) = 'en')\n",
    "  FILTER (CONTAINS(LCASE(?label), \"cyber\") || CONTAINS(LCASE(?label), \"security\") || CONTAINS(LCASE(?label), \"malware\"))\n",
    "}\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "sparql.setQuery(query)\n",
    "sparql.setReturnFormat(JSON)\n",
    "results = sparql.query().convert()\n",
    "\n",
    "# Initialize Knowledge Graph (KG) and portion tracking\n",
    "kg = []\n",
    "portion_indices = {}\n",
    "portion_counter = 1  # Start portion numbering\n",
    "triple_index = 0  # Track overall index\n",
    "\n",
    "print(\"\\nStructured Knowledge Graph:\\n\")\n",
    "\n",
    "for result in results[\"results\"][\"bindings\"]:\n",
    "    concept = result[\"concept\"][\"value\"].split(\"/\")[-1]  # Extracts entity name\n",
    "    label = result[\"label\"][\"value\"]\n",
    "    abstract = result[\"abstract\"][\"value\"]  # Store full abstract without truncation\n",
    "\n",
    "    # Store portion index range\n",
    "    start_index = triple_index\n",
    "    portion_indices[f\"Part {portion_counter}\"] = range(start_index, start_index + 3)  # Each part has 3 triples\n",
    "\n",
    "    # Print structured output\n",
    "    print(f\"\\n# Part {portion_counter}\")\n",
    "    print(f\"({concept}) → (type) → (Software)\")\n",
    "    print(f\"({concept}) → (label) → ({label})\")\n",
    "    print(f\"({concept}) → (abstract) →\")\n",
    "    print(abstract)  # Print full abstract with line breaks\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    # Store in KG\n",
    "    kg.append((concept, \"type\", \"Software\"))\n",
    "    kg.append((concept, \"label\", label))\n",
    "    kg.append((concept, \"abstract\", abstract))  # Store full abstract\n",
    "\n",
    "    # Increment indices\n",
    "    triple_index += 3\n",
    "    portion_counter += 1\n",
    "\n",
    "# Print portion indices separately\n",
    "print(\"\\nPortion Indices:\\n\")\n",
    "for part, index_range in portion_indices.items():\n",
    "    print(f\"{part}: {index_range}\")\n",
    "\n",
    "# Save KG to a text file\n",
    "with open(\"knowledge_graph_output.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for triple in kg:\n",
    "        f.write(f\"( {triple[0]} , {triple[1]} , {triple[2]})\\n\\n\")  # Ensuring full visibility\n",
    "\n",
    "print(\"\\nFinal Knowledge Graph saved as 'knowledge_graph_output.txt'.\")\n",
    "\n",
    "# Print the final KG in a readable format\n",
    "print(\"\\nFinal Knowledge Graph List:\\n\")\n",
    "for triple in kg:\n",
    "  print(\"(\", triple[0],\",\", triple[1],\", \",triple[2], \")\")  # Print without truncation"
   ]
  },
  {
   "cell_type": "raw",
   "id": "818a9197-5541-4b89-ba45-330c0da504a5",
   "metadata": {
    "id": "818a9197-5541-4b89-ba45-330c0da504a5"
   },
   "source": [
    " two different methods for working with knowledge graphs (KGs)\n",
    "\n",
    "| Feature                   | NetworkX (`nx.DiGraph`)                        | Knowledge Graph Framework (`add_triple`)      |\n",
    "|---------------------------|-----------------------------------------------|----------------------------------------------|\n",
    "| **Primary Purpose**       | General-purpose graph operations.             | Semantic knowledge representation and reasoning. |\n",
    "| **Edge Representation**   | Relation stored as edge attribute (`label`).  | Explicitly stores as a `KnowledgeTriple`.    |\n",
    "| **Scalability**           | Efficient for smaller graphs.                 | Often optimized for large-scale KGs.         |\n",
    "| **Functionality**         | Basic graph analysis (e.g., paths, cycles).   | Advanced reasoning, semantic queries, or RAG.|\n",
    "| **Ease of Use**           | Simple and intuitive.                         | May require setup and specific tooling.      |\n",
    "\n",
    "---\n",
    "\n",
    "Which to Use?\n",
    "\n",
    "- Use **NetworkX** if you:\n",
    "  - Need quick, general-purpose graph creation and manipulation.\n",
    "  - Plan to perform basic analysis or visualization.\n",
    "\n",
    "- Use a **dedicated KG framework** if you:\n",
    "  - Are working on semantic reasoning, ontology-based querying, or RAG.\n",
    "  - Need advanced features like integration with NLP pipelines or search engines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94aaed1b-3781-4810-af20-95a5574fb2de",
   "metadata": {
    "id": "94aaed1b-3781-4810-af20-95a5574fb2de"
   },
   "outputs": [],
   "source": [
    "# Function to wrap text for better display\n",
    "def wrap_text(text, width=15):\n",
    "    \"\"\"\n",
    "    Wraps the input text to the specified width.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The text to be wrapped.\n",
    "    - width (int): The maximum number of characters per line.\n",
    "\n",
    "    Returns:\n",
    "    - str: Text wrapped with line breaks.\n",
    "    \"\"\"\n",
    "    return '\\n'.join(textwrap.wrap(text, width))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaebc8f6-31b9-4ad3-9fab-f53132438466",
   "metadata": {
    "id": "aaebc8f6-31b9-4ad3-9fab-f53132438466"
   },
   "source": [
    "\n",
    "Defines a function to perturb the knowledge graph by selectively removing triples\n",
    "belonging to specified parts. This allows testing the impact of missing information\n",
    "on downstream tasks or analysis. The function filters out triples associated with\n",
    "the indices of the parts to be removed and returns the modified knowledge graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "A39WF0Pa_OYY",
   "metadata": {
    "id": "A39WF0Pa_OYY"
   },
   "outputs": [],
   "source": [
    "def perturb_kg_by_removing_parts(kg, parts_to_remove):\n",
    "    \"\"\"\n",
    "    Perturbs the knowledge graph by removing triples from the specified parts.\n",
    "\n",
    "    Parameters:\n",
    "    - kg: The full knowledge graph triples list\n",
    "    - parts_to_remove: List of part names to remove\n",
    "\n",
    "    Returns:\n",
    "    - perturbed_kg: The perturbed KG without the specified parts\n",
    "    \"\"\"\n",
    "    perturbed_kg = []\n",
    "\n",
    "    # Collect indices of the triples to keep based on parts to remove\n",
    "    indices_to_remove = set()\n",
    "    for part in parts_to_remove:\n",
    "        indices_to_remove.update(part_indices[part])\n",
    "\n",
    "    # Add triples that are not in the indices to remove\n",
    "    perturbed_kg = [triple for i, triple in enumerate(kg) if i not in indices_to_remove]\n",
    "\n",
    "    return perturbed_kg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26e290c-6f8a-4e59-ad62-22047f1b5da4",
   "metadata": {
    "id": "c26e290c-6f8a-4e59-ad62-22047f1b5da4"
   },
   "source": [
    "\n",
    "Defines a function to query a GraphQAChain with a question and temperature setting,\n",
    "returning the answer and its embedding. The function initializes the chain with a\n",
    "specified graph and temperature, processes the question, and computes the embedding\n",
    "for the returned answer, facilitating downstream analysis or comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304bd2f5-2511-4fc7-a8e4-1f653baaba33",
   "metadata": {
    "id": "304bd2f5-2511-4fc7-a8e4-1f653baaba33"
   },
   "outputs": [],
   "source": [
    "def get_answer_and_embedding(question: str, temp: float, graph):\n",
    "    \"\"\"\n",
    "    Sends a question and temperature to the GraphQAChain and returns the original answer string\n",
    "    and its embedding as separate outputs.\n",
    "\n",
    "    Args:\n",
    "        question (str): The question to ask the chain.\n",
    "        temp (float): The temperature setting for the OpenAI model.\n",
    "        graph: The graph object for the GraphQAChain.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str, list]: The original answer as a string and its embedding as a list.\n",
    "    \"\"\"\n",
    "    # Initialize the GraphQAChain with the specified temperature\n",
    "    chain = GraphQAChain.from_llm(OpenAI(temperature=temp), graph=graph, verbose=False)\n",
    "\n",
    "    # Run the question through the chain to get the answer\n",
    "    original_answer = chain.run(question)\n",
    "    original_answer_str = str(original_answer)\n",
    "\n",
    "    # Compute the embedding for the original answer\n",
    "    original_answer_embedding = get_embedding(original_answer)\n",
    "\n",
    "    # Return both answer and embedding separately\n",
    "    return original_answer_str, original_answer_embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ed4342-7303-4b29-9321-2fa8e2404d70",
   "metadata": {
    "id": "53ed4342-7303-4b29-9321-2fa8e2404d70"
   },
   "source": [
    "This function computes the embedding for a given text using a specified model.\n",
    "It processes the text by removing newline characters and queries the OpenAI\n",
    "embeddings API to generate a vector representation, useful for similarity\n",
    "comparisons and downstream tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f07e587-90db-4a9e-b24d-79ec27ea0abb",
   "metadata": {
    "id": "7f07e587-90db-4a9e-b24d-79ec27ea0abb"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import unicodedata\n",
    "import re\n",
    "embedding_cache = {}\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    Normalize text by removing excessive spaces, normalizing Unicode characters,\n",
    "    and converting to lowercase.\n",
    "    \"\"\"\n",
    "    text = text.replace(\"\\n\", \" \").strip()  # Remove newlines and extra spaces\n",
    "    text = unicodedata.normalize(\"NFKC\", text)  # Normalize Unicode characters\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Replace multiple spaces with a single space\n",
    "    text = text.lower()  # Convert to lowercase (optional but recommended)\n",
    "    return text\n",
    "\n",
    "def get_embedding(text):\n",
    "    text = normalize_text(text)\n",
    "    if text in embedding_cache:\n",
    "        return embedding_cache[text]  # Return cached embedding\n",
    "    embedding = client.embeddings.create(input=[text], model=EMBEDDING_MODEL).data[0].embedding\n",
    "    embedding_cache[text] = embedding  # Store result in cache\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d78ec0-492e-4bac-ba65-53b80a150be4",
   "metadata": {
    "id": "d6d78ec0-492e-4bac-ba65-53b80a150be4"
   },
   "source": [
    "This function visualizes the explainability of a knowledge graph by displaying the original graph and an\n",
    "enhanced graph with nodes and edges colored based on their importance coefficients. It leverages a directed\n",
    "graph structure, wraps node labels for readability, adjusts node sizes based on connectivity, and applies a\n",
    "custom colormap to represent the significance of graph components. The visualization is presented in a\n",
    "two-panel layout, highlighting both the original structure and the explainability features derived from\n",
    "Simple SMILE GraphRAG analysis. A color bar provides a reference for importance coefficients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5531a7-13ac-4a3c-9646-cbe61bcb5499",
   "metadata": {
    "id": "fd5531a7-13ac-4a3c-9646-cbe61bcb5499"
   },
   "outputs": [],
   "source": [
    "def wrap_text(node1, relation, node2, max_words=8):\n",
    "    \"\"\"Wrap text if it contains more than `max_words` words and append '_explanation' to abstract nodes.\"\"\"\n",
    "    if relation == \"abstract\":\n",
    "        words = node2.split()\n",
    "        return f\"{node1}_explanation\" if len(words) > max_words else node2\n",
    "    return node2\n",
    "\n",
    "def build_graph(kg, coeff, part_indices):\n",
    "    \"\"\"Helper function to build graph, assign colors, and sizes.\"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    for node1, relation, node2 in kg:\n",
    "        wrapped_node2 = wrap_text(node1, relation, node2)\n",
    "        G.add_edge(node1, wrapped_node2, label=relation)\n",
    "\n",
    "    pos = nx.spring_layout(G, k=8, iterations=200, seed=0)\n",
    "    cmap = mcolors.LinearSegmentedColormap.from_list('red_blue', ['blue', '#d3d3d3', 'red'])\n",
    "    norm = mcolors.Normalize(vmin=-1, vmax=1)\n",
    "    node_sizes = [1500 + 100 * G.degree(node) for node in G.nodes()]\n",
    "\n",
    "    node_colors = []\n",
    "    for node in G.nodes():\n",
    "        for part_name, indices in part_indices.items():\n",
    "            part_idx = int(part_name.split()[-1]) - 1\n",
    "            coeff_value = coeff[part_idx]\n",
    "            color = cmap(norm(coeff_value))\n",
    "            if any(i < len(kg) and (node == kg[i][0] or node == wrap_text(kg[i][0], kg[i][1], kg[i][2])) for i in indices):\n",
    "                node_colors.append(color)\n",
    "                break\n",
    "        else:\n",
    "            node_colors.append('#8da0cb')\n",
    "\n",
    "    edge_colors = []\n",
    "    for i, (node1, node2) in enumerate(G.edges()):\n",
    "        for part_name, indices in part_indices.items():\n",
    "            part_idx = int(part_name.split()[-1]) - 1\n",
    "            coeff_value = coeff[part_idx]\n",
    "            color = cmap(norm(coeff_value))\n",
    "            if i in indices:\n",
    "                edge_colors.append(color)\n",
    "                break\n",
    "        else:\n",
    "            edge_colors.append('gray')\n",
    "\n",
    "    return G, pos, node_sizes, node_colors, edge_colors\n",
    "\n",
    "def plot_knowledge_graph_explainability(kg, part_indices, coeff):\n",
    "    \"\"\"\n",
    "    Improved visualization of a knowledge graph with explainability features.\n",
    "\n",
    "    Parameters:\n",
    "        kg (list): Knowledge graph triplets (node1, relation, node2).\n",
    "        part_indices (dict): Mapping of part names to indices.\n",
    "        coeff (list): Importance coefficients for each part.\n",
    "    \"\"\"\n",
    "    G, pos, node_sizes, node_colors, edge_colors = build_graph(kg, coeff, part_indices)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 8), dpi=300)\n",
    "\n",
    "    nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=node_sizes, ax=ax)\n",
    "    nx.draw_networkx_edges(G, pos, edge_color=edge_colors, width=1.5, ax=ax)\n",
    "    nx.draw_networkx_labels(G, pos, font_size=6, ax=ax)\n",
    "    edge_labels = nx.get_edge_attributes(G, 'label')\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6, ax=ax)\n",
    "    ax.set_title(\"Knowledge Graph Explainability\", fontsize=14)\n",
    "    ax.axis('off')\n",
    "\n",
    "    cmap = mcolors.LinearSegmentedColormap.from_list('red_blue', ['blue', '#d3d3d3', 'red'])\n",
    "    norm = mcolors.Normalize(vmin=-1, vmax=1)\n",
    "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "    sm.set_array([])\n",
    "    fig.colorbar(sm, ax=ax, orientation='horizontal', label='Importance Coefficients', fraction=0.03, pad=0.05)\n",
    "\n",
    "    plt.savefig('knowledge_graph_explainability.png', bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7988ea2a-af8a-4089-8e47-8a61ecc7b86d",
   "metadata": {
    "id": "7988ea2a-af8a-4089-8e47-8a61ecc7b86d"
   },
   "source": [
    "This function computes and displays fidelity metrics for evaluating regression model performance,\n",
    "including standard measures like Mean Squared Error (MSE), R-squared (R²), and Mean Absolute Error (MAE).\n",
    "It also calculates advanced metrics such as weighted R², weighted adjusted R², mean losses (L1 and L2),\n",
    "and weighted losses. These metrics provide a comprehensive assessment of model fidelity, considering\n",
    "weights and coefficients for a nuanced evaluation of the regression results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f98cf9e-9a2f-40d2-ae4e-94420ae277dd",
   "metadata": {
    "id": "1f98cf9e-9a2f-40d2-ae4e-94420ae277dd"
   },
   "outputs": [],
   "source": [
    "def calculate_fidelity_metrics(y_true, y_pred, weights, coeff):\n",
    "    \"\"\"\n",
    "    Calculate and print various fidelity metrics for a regression model.\n",
    "\n",
    "    Parameters:\n",
    "        y_true (array-like): True values (ground truth).\n",
    "        y_pred (array-like): Predicted values.\n",
    "        weights (array-like): Sample weights.\n",
    "        coeff (array-like): Coefficients of the regression model (used for adjusted R²).\n",
    "    \"\"\"\n",
    "    # Calculate regression metrics\n",
    "    mse = mean_squared_error(y_true, y_pred, sample_weight=weights)\n",
    "    r2 = r2_score(y_true, y_pred, sample_weight=weights)\n",
    "    mae = mean_absolute_error(y_true, y_pred, sample_weight=weights)\n",
    "\n",
    "    # Mean loss (Lm)\n",
    "    mean_loss_f = np.mean(y_true)\n",
    "    mean_loss_g = np.mean(y_pred)\n",
    "    mean_loss = abs(mean_loss_f - mean_loss_g)\n",
    "\n",
    "    # Mean L1 and L2 loss\n",
    "    mean_l1 = np.mean(np.abs(y_true - y_pred))\n",
    "    mean_l2 = np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "    # Weighted L1 and L2 loss\n",
    "    n = len(y_true)\n",
    "    weighted_l1 = np.sum(weights * np.abs(y_true - y_pred)) / n\n",
    "    weighted_l2 = np.sum(weights * (y_true - y_pred) ** 2) / n\n",
    "\n",
    "    # Weighted R²\n",
    "    f_mean = np.average(y_true, weights=weights)\n",
    "    ss_tot = np.sum(weights * (y_true - f_mean) ** 2)\n",
    "    ss_res = np.sum(weights * (y_true - y_pred) ** 2)\n",
    "    weighted_r2 = 1 - ss_res / ss_tot\n",
    "\n",
    "    # Weighted adjusted R²\n",
    "    p = len(coeff)\n",
    "    weighted_adj_r2 = 1 - (1 - weighted_r2) * (n - 1) / (n - p - 1)\n",
    "\n",
    "    # Print fidelity metrics\n",
    "    print(100 * '-')\n",
    "    print('Fidelity:')\n",
    "    print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "    print(f\"R-squared (R²): {r2}\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "    print(f\"Mean Loss (Lm): {mean_loss}\")\n",
    "    print(f\"Mean L1 Loss: {mean_l1}\")\n",
    "    print(f\"Mean L2 Loss: {mean_l2}\")\n",
    "    print(f\"Weighted L1 Loss: {weighted_l1}\")\n",
    "    print(f\"Weighted L2 Loss: {weighted_l2}\")\n",
    "    print(f\"Weighted R-squared (R²ω): {weighted_r2}\")\n",
    "    print(f\"Weighted Adjusted R-squared (Rˆ²ω): {weighted_adj_r2}\")\n",
    "    print(100 * '-')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037ebf4d-f5e5-4422-9fc5-4051c53a0ed5",
   "metadata": {
    "id": "037ebf4d-f5e5-4422-9fc5-4051c53a0ed5"
   },
   "source": [
    "This function generates a scatter plot comparing actual (true) values to predicted values,\n",
    "with an optional weighting mechanism to scale point sizes. It includes a perfect prediction\n",
    "line (y = x) for reference and displays the R² score as a measure of model performance. The\n",
    "plot provides an intuitive visual assessment of the alignment between predictions and ground\n",
    "truth, with customizable point size based on weights for better visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3848a56b-a0ca-49ee-bea1-a2f082b6d8a6",
   "metadata": {
    "id": "3848a56b-a0ca-49ee-bea1-a2f082b6d8a6"
   },
   "outputs": [],
   "source": [
    "def plot_actual_vs_predicted(y_true, y_pred, weights=None):\n",
    "    \"\"\"\n",
    "    Plot actual vs. predicted values with an optional weight normalization for point sizes.\n",
    "    Displays the R² score and a perfect prediction line (y = x).\n",
    "\n",
    "    Parameters:\n",
    "        y_true (array-like): True values (ground truth).\n",
    "        y_pred (array-like): Predicted values.\n",
    "        weights (array-like, optional): Weights for scaling point sizes in the scatter plot.\n",
    "    \"\"\"\n",
    "    # Calculate the R² score\n",
    "    r2 = r2_score(y_true, y_pred, sample_weight=weights)\n",
    "\n",
    "    # Normalize weights for better visualization\n",
    "    if weights is not None:\n",
    "        normalized_weights = np.array(weights) / np.max(weights) * 100  # Scale weights to a reasonable range\n",
    "    else:\n",
    "        normalized_weights = 50\n",
    "\n",
    "    # Plotting\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(y_true, y_pred, s=normalized_weights, label='Data points', alpha=0.6)  # Use weights for point sizes\n",
    "\n",
    "    # Determine the range for the perfect prediction line\n",
    "    min_val = min(y_true.min(), y_pred.min())\n",
    "    max_val = max(y_true.max(), y_pred.max())\n",
    "\n",
    "    # Plotting the Perfect Prediction Line (y = x)\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], 'k--', lw=2, label='Perfect Prediction Line')\n",
    "\n",
    "    # Set the plot limits to better frame the data\n",
    "    ax.set_xlim([min_val, max_val])\n",
    "    ax.set_ylim([min_val, max_val])\n",
    "\n",
    "    # Labeling the axes\n",
    "    ax.set_xlabel('Actual')\n",
    "    ax.set_ylabel('Predicted')\n",
    "\n",
    "    # Title with R² score rounded to two decimal places\n",
    "    ax.set_title(f'Actual vs. Predicted Values\\nR²: {r2:.2f}')\n",
    "\n",
    "    # Show legend\n",
    "    ax.legend()\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cab87ae-283f-418c-b161-0869d2919aa9",
   "metadata": {
    "id": "4cab87ae-283f-418c-b161-0869d2919aa9"
   },
   "source": [
    "Defines the question to query the GraphQAChain or knowledge retrieval system.\n",
    "Here, the question \"What is RAG?\" seeks information about Retrieval-Augmented Generation,\n",
    "a framework that integrates external knowledge bases to improve the accuracy and reliability\n",
    "of AI-generated responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quDGLDk-8jef",
   "metadata": {
    "id": "quDGLDk-8jef"
   },
   "outputs": [],
   "source": [
    "part_indices ={\n",
    "\"Part 1\": range(0, 3),\n",
    "\"Part 2\": range(3, 6),\n",
    "\"Part 3\": range(6, 9),\n",
    "\"Part 4\": range(9, 12),\n",
    "\"Part 5\": range(12, 15),\n",
    "\"Part 6\": range(15, 18),\n",
    "\"Part 7\": range(18, 21),\n",
    "\"Part 8\": range(21, 24),\n",
    "\"Part 9\": range(24, 27),\n",
    "\"Part 10\": range(27, 30)\n",
    "}\n",
    "part_names = list(part_indices.keys())\n",
    "\n",
    "# Instantiate the graph\n",
    "graph = NetworkxEntityGraph()\n",
    "\n",
    "# Build the graph from the knowledge triples\n",
    "for (node1, relation, node2) in kg:\n",
    "    graph.add_triple(KnowledgeTriple(node1, relation, node2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23870dad-5d4b-4e18-b06d-b72c74fe579f",
   "metadata": {
    "id": "23870dad-5d4b-4e18-b06d-b72c74fe579f"
   },
   "outputs": [],
   "source": [
    "question = \"what is Network_Security_Services?\"\n",
    "#Portion 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a5cc60-d3eb-4d0e-91b4-f2c2a10028c5",
   "metadata": {
    "id": "17a5cc60-d3eb-4d0e-91b4-f2c2a10028c5"
   },
   "source": [
    "This snippet sets the temperature parameter to 0 for deterministic response generation and\n",
    "queries the GraphQAChain with the question, \"What is RAG?\". The function `get_answer_and_embedding`\n",
    "returns the original answer as a string along with its embedding. The answer is then printed for review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10979ac4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "10979ac4",
    "outputId": "76b88be9-6adf-45d9-a9a7-69322b712ded"
   },
   "outputs": [],
   "source": [
    "temp = 0\n",
    "original_answer_str, original_answer_embedding = get_answer_and_embedding(question, temp, graph)\n",
    "print(original_answer_str)\n",
    "# Compute the embedding for the original answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caab5a60-2b82-4f71-ba5d-b97972336298",
   "metadata": {
    "id": "caab5a60-2b82-4f71-ba5d-b97972336298"
   },
   "outputs": [],
   "source": [
    "# Define the original vector (all parts present)\n",
    "original = np.array([1, 1, 1, 1, 1,1, 1, 1, 1, 1])\n",
    "original = original.reshape(1, -1)  # Shape becomes (1,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "H_wSw23WDuBd",
   "metadata": {
    "id": "H_wSw23WDuBd"
   },
   "source": [
    "This code defines a custom function, timed_and_log_detailed, to measure the execution time of a code block, including CPU usage (user, system, and total CPU times) and wall clock time. It uses the psutil library for CPU time tracking and the time module for wall time measurement. The timing results are stored in a global runtimes list as dictionaries for later analysis and are also printed in a detailed format after execution. This is useful for profiling the performance of specific code segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IUMzq9jDDtWM",
   "metadata": {
    "id": "IUMzq9jDDtWM"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import psutil\n",
    "from IPython.core.magic import register_cell_magic\n",
    "\n",
    "# Initialize a list to store runtimes\n",
    "runtimes = []\n",
    "\n",
    "@register_cell_magic\n",
    "def timed_and_log_detailed(line, cell):\n",
    "    \"\"\"\n",
    "    Custom magic to measure CPU times and wall time for a cell\n",
    "    and log the results in a global runtimes list.\n",
    "    \"\"\"\n",
    "    global runtimes\n",
    "\n",
    "    # Record the start time and CPU usage\n",
    "    start_wall_time = time.time()\n",
    "    start_cpu_time = psutil.cpu_times()\n",
    "\n",
    "    # Execute the cell\n",
    "    exec(cell, globals())\n",
    "\n",
    "    # Record the end time and CPU usage\n",
    "    end_wall_time = time.time()\n",
    "    end_cpu_time = psutil.cpu_times()\n",
    "\n",
    "    # Calculate times\n",
    "    wall_time = end_wall_time - start_wall_time\n",
    "    user_time = end_cpu_time.user - start_cpu_time.user\n",
    "    sys_time = end_cpu_time.system - start_cpu_time.system\n",
    "    total_cpu_time = user_time + sys_time\n",
    "\n",
    "    # Log the times as a dictionary\n",
    "    runtimes.append({\n",
    "        \"user_time\": user_time,\n",
    "        \"sys_time\": sys_time,\n",
    "        \"total_cpu_time\": total_cpu_time,\n",
    "        \"wall_time\": wall_time\n",
    "    })\n",
    "\n",
    "    # Print detailed runtime\n",
    "    print(f\"CPU times: user {user_time:.2f} s, sys {sys_time:.2f} s, total {total_cpu_time:.2f} s\")\n",
    "    print(f\"Wall time: {wall_time:.2f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041524b2-b927-40a7-a879-6ecdadea41a1",
   "metadata": {
    "id": "041524b2-b927-40a7-a879-6ecdadea41a1"
   },
   "source": [
    "# Exploring Explainability: Fidelity Metrics Comparison for Text vs. Text and Graph vs. Graph Representations Using Cosine, wasserstein_distance, and Combined Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93af9b8-a018-4713-98c3-eb79012f7a81",
   "metadata": {
    "id": "b93af9b8-a018-4713-98c3-eb79012f7a81"
   },
   "source": [
    "This script evaluates the robustness of a knowledge graph system by perturbing the graph and measuring the\n",
    "impact on the generated responses. It iteratively removes random parts of the graph, calculates similarity\n",
    "metrics (Wasserstein distance and cosine similarity) between the original and perturbed response embeddings,\n",
    "and stores the perturbation details for analysis. Additionally, it computes cosine distances and kernel-based\n",
    "weights for further analysis of the perturbations' impact.\n",
    "\n",
    "Key Steps:\n",
    "1. Generate perturbed versions of the knowledge graph by randomly removing parts.\n",
    "2. Create embeddings for responses from the perturbed graphs.\n",
    "3. Calculate similarity metrics between original and perturbed responses.\n",
    "4. Compute kernel-based weights using cosine distances for downstream tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0f3725-e584-4974-b7fd-93be3b552069",
   "metadata": {
    "id": "ce0f3725-e584-4974-b7fd-93be3b552069"
   },
   "source": [
    "# BayLIME: Dual Metrics: Blending Inverse Wasserstein Distance and Cosine Similarity for Text and Graph-to-Graph using Cosine\n",
    "Cosine Similarity: Measuring Alignment and Fidelity Between Textual Representations\n",
    "Wasserstein Distance: Evaluating Semantic Shifts Between Textual Representations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7530da44-237f-4925-ae3e-d0f532f3f418",
   "metadata": {
    "id": "7530da44-237f-4925-ae3e-d0f532f3f418"
   },
   "source": [
    "This script computes a combined similarity metric by scaling inverse Wasserstein distances\n",
    "(to normalize between 0 and 1) and adding them to cosine similarities. The result, stored\n",
    "in `Similarities_`, provides a composite measure of fidelity, highlighting the alignment\n",
    "between perturbed and original responses in the knowledge graph system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8rHMW4hVeVhG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "8rHMW4hVeVhG",
    "outputId": "7cb8f5e4-0993-4cf1-d5a3-9bae533abaae"
   },
   "outputs": [],
   "source": [
    "%%timed_and_log_detailed\n",
    "similarities_wd = []\n",
    "similarities_cosine = []\n",
    "perturbations_vect2 = []\n",
    "perturbation_texts = []  # Store the perturbation texts\n",
    "\n",
    "# Loop for perturbations\n",
    "for i in range(20):\n",
    "    # Make a copy of the original vector for each iteration\n",
    "    perturbation_vector = original.copy().flatten()\n",
    "\n",
    "    # Randomly choose one or more parts to remove\n",
    "    num_parts_to_remove = random.randint(1, len(part_names))\n",
    "    parts_to_remove_indices = random.sample(range(len(part_names)), num_parts_to_remove)\n",
    "\n",
    "    # Set the selected parts to 0 in the perturbation vector\n",
    "    for part_idx in parts_to_remove_indices:\n",
    "        perturbation_vector[part_idx] = 0\n",
    "\n",
    "    # Append the perturbation vector to perturbations_vect2\n",
    "    perturbations_vect2.append(perturbation_vector)\n",
    "\n",
    "    # Perturb the KG by removing the selected parts\n",
    "    parts_to_remove = [part_names[idx] for idx in parts_to_remove_indices]\n",
    "    perturbed_kg = perturb_kg_by_removing_parts(kg, parts_to_remove)\n",
    "\n",
    "    # Create a temporary graph for the perturbed KG\n",
    "    graph_temp = NetworkxEntityGraph()\n",
    "    for (node1, relation, node2) in perturbed_kg:\n",
    "        graph_temp.add_triple(KnowledgeTriple(node1, relation, node2))\n",
    "\n",
    "    # Generate response using GraphQAChain\n",
    "    chain = GraphQAChain.from_llm(OpenAI(temperature=0), graph=graph_temp, verbose=False)\n",
    "    temp_response = chain.run(question)\n",
    "\n",
    "    # Store the perturbed response text\n",
    "    perturbation_texts.append(temp_response)\n",
    "\n",
    "    # Get embedding for the perturbed response\n",
    "    temp_response_embedding = get_embedding(temp_response)\n",
    "\n",
    "\n",
    "    # Calculate Wasserstein distance between the original and perturbed responses\n",
    "    similarity_wd = wasserstein_distance(original_answer_embedding, temp_response_embedding)\n",
    "    similarities_wd.append(similarity_wd)\n",
    "\n",
    "    # Calculate cosine similarity between the original and perturbed responses\n",
    "    similarity_cosine = 1 - cosine(original_answer_embedding, temp_response_embedding)\n",
    "    similarities_cosine.append(similarity_cosine)\n",
    "\n",
    "    print(f\"Iteration {i + 1}\")\n",
    "    print(f\"Parts removed: {parts_to_remove}\")\n",
    "    print(f\"original_answer response: {original_answer_str}\")\n",
    "    print(f\"Perturbed response: {temp_response}\")\n",
    "    print(f\"Wasserstein Distance with original answer: {similarity_wd}\")\n",
    "    print(f\"Cosine Similarity with original answer: {similarity_cosine}\\n\")\n",
    "\n",
    "# Convert perturbations_vect2 to a numpy array for pairwise distance calculation\n",
    "perturbations_vect2 = np.array(perturbations_vect2)\n",
    "\n",
    "# Calculate cosine distances between perturbation vectors and the original vector\n",
    "distances = sklearn.metrics.pairwise_distances(perturbations_vect2, original, metric='cosine').ravel()\n",
    "\n",
    "# Assuming you may use kernel width in further computations\n",
    "kernel_width = 0.25\n",
    "weights = np.sqrt(np.exp(-(distances**2)/kernel_width**2))\n",
    "\n",
    "# Print all similarities and weights\n",
    "print(f\"Wasserstein Distances: {similarities_wd}\")\n",
    "print(f\"Cosine Similarities: {similarities_cosine}\")\n",
    "print(f\"Weights: {weights}\")\n",
    "\n",
    "# Optionally print all perturbation texts together for a consolidated view\n",
    "print(\"\\n--- Summary of Perturbations ---\")\n",
    "for i, text in enumerate(perturbation_texts):\n",
    "    print(f\"Perturbation {i + 1}: {text}\")#bayLime_sum_inv_wd_cosine\n",
    "epsilon = 1e-6\n",
    "\n",
    "# Calculate the inverse of each Wasserstein distance, adding a small epsilon to avoid division by zero\n",
    "inverse_similarities_wd = [1.0 / (dist + epsilon) for dist in similarities_wd]\n",
    "\n",
    "# Find the minimum and maximum of the inverse Wasserstein distances\n",
    "min_value = min(inverse_similarities_wd)\n",
    "max_value = max(inverse_similarities_wd)\n",
    "\n",
    "# Scale inverse Wasserstein distances between 0 and 1\n",
    "scaled_similarities_wd = [(value - min_value) / (max_value - min_value) for value in inverse_similarities_wd]\n",
    "\n",
    "# Combine the scaled inverse Wasserstein distances and cosine similarities\n",
    "Similarities_ = [wd + cos for wd, cos in zip(scaled_similarities_wd, similarities_cosine)]\n",
    "# Print the combined list of similarities\n",
    "print(Similarities_)\n",
    "simpler_model = BayesianRidge()\n",
    "simpler_model.fit(X=perturbations_vect2, y= Similarities_, sample_weight=weights)\n",
    "coeff = simpler_model.coef_\n",
    "# Call the function\n",
    "plot_knowledge_graph_explainability(kg, part_indices, coeff)\n",
    "y_true = np.array(Similarities_).ravel()\n",
    "y_pred = simpler_model.predict(perturbations_vect2).ravel()\n",
    "\n",
    "# Call the function\n",
    "calculate_fidelity_metrics(y_true, y_pred, weights, coeff)\n",
    "# Call the function\n",
    "plot_actual_vs_predicted(y_true, y_pred, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f32ec4f-7931-48b1-b1b8-b3ee4f3a7ad0",
   "metadata": {
    "id": "0f32ec4f-7931-48b1-b1b8-b3ee4f3a7ad0"
   },
   "source": [
    "This code trains a Bayesian Ridge regression model to learn the relationship between perturbation vectors\n",
    "(`perturbations_vect2`) and the combined similarity metric (`Similarities_`). The model incorporates sample\n",
    "weights (`weights`) to prioritize certain data points. After training, the coefficients (`coeff`) of the model\n",
    "are extracted, which represent the importance of each feature in predicting the similarity metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b37c87-ab02-4b88-955d-60ee1da1a371",
   "metadata": {
    "id": "c0b37c87-ab02-4b88-955d-60ee1da1a371"
   },
   "source": [
    "This code defines the importance coefficients (`coeff`) for different parts of the knowledge graph\n",
    "and visualizes their explainability using the `plot_knowledge_graph_explainability` function. The\n",
    "coefficients indicate the contribution of each part to the overall fidelity, and the visualization\n",
    "highlights these contributions through node and edge color mappings in the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8830277-4cb3-41bc-8be9-fecda605094a",
   "metadata": {
    "id": "a8830277-4cb3-41bc-8be9-fecda605094a"
   },
   "source": [
    "This code calculates fidelity metrics to evaluate the performance of the Bayesian Ridge regression model.\n",
    "The true similarity values (`y_true`) and predicted values (`y_pred`) are passed to the\n",
    "`calculate_fidelity_metrics` function, along with sample weights (`weights`) and model coefficients (`coeff`).\n",
    "The function outputs metrics like MSE, R², weighted R², and other advanced measures for a comprehensive\n",
    "assessment of the model's fidelity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cf944e-bfbc-492d-b84e-7a49c932ec2a",
   "metadata": {
    "id": "f2cf944e-bfbc-492d-b84e-7a49c932ec2a"
   },
   "source": [
    "This code generates a scatter plot to visually compare the true similarity values (`y_true`) against the\n",
    "predicted values (`y_pred`). The function `plot_actual_vs_predicted` scales point sizes based on sample\n",
    "weights (`weights`) and includes a reference line for perfect predictions (y = x). It also displays the\n",
    "R² score to quantify the model's predictive accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2a783f",
   "metadata": {
    "id": "7d2a783f"
   },
   "source": [
    "# BayLIME: Analyzing Fidelity: Comparing Text and Graph Representations Using Cosine Similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da0fc4a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6da0fc4a",
    "outputId": "dd8672a0-5b52-4327-d3c4-ff4e61d399d0"
   },
   "outputs": [],
   "source": [
    "%%timed_and_log_detailed\n",
    "similarities_wd = []\n",
    "similarities_cosine = []\n",
    "perturbations_vect2 = []\n",
    "perturbation_texts = []  # Store the perturbation texts\n",
    "\n",
    "# Loop for perturbations\n",
    "for i in range(20):\n",
    "    # Make a copy of the original vector for each iteration\n",
    "    perturbation_vector = original.copy().flatten()\n",
    "\n",
    "    # Randomly choose one or more parts to remove\n",
    "    num_parts_to_remove = random.randint(1, len(part_names))\n",
    "    parts_to_remove_indices = random.sample(range(len(part_names)), num_parts_to_remove)\n",
    "\n",
    "    # Set the selected parts to 0 in the perturbation vector\n",
    "    for part_idx in parts_to_remove_indices:\n",
    "        perturbation_vector[part_idx] = 0\n",
    "\n",
    "    # Append the perturbation vector to perturbations_vect2\n",
    "    perturbations_vect2.append(perturbation_vector)\n",
    "\n",
    "    # Perturb the KG by removing the selected parts\n",
    "    parts_to_remove = [part_names[idx] for idx in parts_to_remove_indices]\n",
    "    perturbed_kg = perturb_kg_by_removing_parts(kg, parts_to_remove)\n",
    "\n",
    "    # Create a temporary graph for the perturbed KG\n",
    "    graph_temp = NetworkxEntityGraph()\n",
    "    for (node1, relation, node2) in perturbed_kg:\n",
    "        graph_temp.add_triple(KnowledgeTriple(node1, relation, node2))\n",
    "\n",
    "    # Generate response using GraphQAChain\n",
    "    chain = GraphQAChain.from_llm(OpenAI(temperature=0), graph=graph_temp, verbose=False)\n",
    "    temp_response = chain.run(question)\n",
    "\n",
    "    # Store the perturbed response text\n",
    "    perturbation_texts.append(temp_response)\n",
    "\n",
    "    # Get embedding for the perturbed response\n",
    "    temp_response_embedding = get_embedding(temp_response)\n",
    "\n",
    "    # Calculate cosine similarity between the original and perturbed responses\n",
    "    similarity_cosine = 1 - cosine(original_answer_embedding, temp_response_embedding)\n",
    "    similarities_cosine.append(similarity_cosine)\n",
    "\n",
    "    print(f\"Iteration {i + 1}\")\n",
    "    print(f\"Parts removed: {parts_to_remove}\")\n",
    "    print(f\"original_answer response: {original_answer_str}\")\n",
    "    print(f\"Perturbed response: {temp_response}\")\n",
    "    print(f\"Wasserstein Distance with original answer: {similarity_wd}\")\n",
    "    print(f\"Cosine Similarity with original answer: {similarity_cosine}\\n\")\n",
    "\n",
    "# Convert perturbations_vect2 to a numpy array for pairwise distance calculation\n",
    "perturbations_vect2 = np.array(perturbations_vect2)\n",
    "\n",
    "# Calculate cosine distances between perturbation vectors and the original vector\n",
    "distances = sklearn.metrics.pairwise_distances(perturbations_vect2, original, metric='cosine').ravel()\n",
    "\n",
    "# Assuming you may use kernel width in further computations\n",
    "kernel_width = 0.25\n",
    "weights = np.sqrt(np.exp(-(distances**2)/kernel_width**2))\n",
    "\n",
    "# Print all similarities and weights\n",
    "print(f\"Wasserstein Distances: {similarities_wd}\")\n",
    "print(f\"Cosine Similarities: {similarities_cosine}\")\n",
    "print(f\"Weights: {weights}\")\n",
    "\n",
    "# Optionally print all perturbation texts together for a consolidated view\n",
    "print(\"\\n--- Summary of Perturbations ---\")\n",
    "for i, text in enumerate(perturbation_texts):\n",
    "    print(f\"Perturbation {i + 1}: {text}\")\n",
    "simpler_model =  BayesianRidge()\n",
    "simpler_model.fit(X=perturbations_vect2, y=similarities_cosine, sample_weight=weights)\n",
    "coeff = simpler_model.coef_\n",
    "# Call the function\n",
    "plot_knowledge_graph_explainability(kg, part_indices, coeff)\n",
    "y_true = np.array(similarities_cosine).ravel()\n",
    "y_pred = simpler_model.predict(perturbations_vect2).ravel()\n",
    "\n",
    "# Call the function\n",
    "calculate_fidelity_metrics(y_true, y_pred, weights, coeff)\n",
    "# Call the function\n",
    "plot_actual_vs_predicted(y_true, y_pred, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c017042",
   "metadata": {
    "id": "1c017042"
   },
   "source": [
    "# BayLIME: Fidelity Analysis of Text-to-Text using Inverse Wasserstein Distance and Graph-to-Graph using Cosine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affe341f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "affe341f",
    "outputId": "ed0dd41e-3b9c-4d45-96c8-4b172aab35f1"
   },
   "outputs": [],
   "source": [
    "%%timed_and_log_detailed\n",
    "similarities_wd = []\n",
    "similarities_cosine = []\n",
    "perturbations_vect2 = []\n",
    "perturbation_texts = []  # Store the perturbation texts\n",
    "\n",
    "# Loop for perturbations\n",
    "for i in range(20):\n",
    "    # Make a copy of the original vector for each iteration\n",
    "    perturbation_vector = original.copy().flatten()\n",
    "\n",
    "    # Randomly choose one or more parts to remove\n",
    "    num_parts_to_remove = random.randint(1, len(part_names))\n",
    "    parts_to_remove_indices = random.sample(range(len(part_names)), num_parts_to_remove)\n",
    "\n",
    "    # Set the selected parts to 0 in the perturbation vector\n",
    "    for part_idx in parts_to_remove_indices:\n",
    "        perturbation_vector[part_idx] = 0\n",
    "\n",
    "    # Append the perturbation vector to perturbations_vect2\n",
    "    perturbations_vect2.append(perturbation_vector)\n",
    "\n",
    "    # Perturb the KG by removing the selected parts\n",
    "    parts_to_remove = [part_names[idx] for idx in parts_to_remove_indices]\n",
    "    perturbed_kg = perturb_kg_by_removing_parts(kg, parts_to_remove)\n",
    "\n",
    "    # Create a temporary graph for the perturbed KG\n",
    "    graph_temp = NetworkxEntityGraph()\n",
    "    for (node1, relation, node2) in perturbed_kg:\n",
    "        graph_temp.add_triple(KnowledgeTriple(node1, relation, node2))\n",
    "\n",
    "    # Generate response using GraphQAChain\n",
    "    chain = GraphQAChain.from_llm(OpenAI(temperature=0), graph=graph_temp, verbose=False)\n",
    "    temp_response = chain.run(question)\n",
    "\n",
    "    # Store the perturbed response text\n",
    "    perturbation_texts.append(temp_response)\n",
    "\n",
    "    # Get embedding for the perturbed response\n",
    "    temp_response_embedding = get_embedding(temp_response)\n",
    "\n",
    "\n",
    "    # Calculate Wasserstein distance between the original and perturbed responses\n",
    "    similarity_wd = wasserstein_distance(original_answer_embedding, temp_response_embedding)\n",
    "    similarities_wd.append(similarity_wd)\n",
    "\n",
    "    # Calculate cosine similarity between the original and perturbed responses\n",
    "    similarity_cosine = 1 - cosine(original_answer_embedding, temp_response_embedding)\n",
    "    similarities_cosine.append(similarity_cosine)\n",
    "\n",
    "    print(f\"Iteration {i + 1}\")\n",
    "    print(f\"Parts removed: {parts_to_remove}\")\n",
    "    print(f\"original_answer response: {original_answer_str}\")\n",
    "    print(f\"Perturbed response: {temp_response}\")\n",
    "    print(f\"Wasserstein Distance with original answer: {similarity_wd}\")\n",
    "    print(f\"Cosine Similarity with original answer: {similarity_cosine}\\n\")\n",
    "\n",
    "# Convert perturbations_vect2 to a numpy array for pairwise distance calculation\n",
    "perturbations_vect2 = np.array(perturbations_vect2)\n",
    "\n",
    "# Calculate cosine distances between perturbation vectors and the original vector\n",
    "distances = sklearn.metrics.pairwise_distances(perturbations_vect2, original, metric='cosine').ravel()\n",
    "\n",
    "# Assuming you may use kernel width in further computations\n",
    "kernel_width = 0.25\n",
    "weights = np.sqrt(np.exp(-(distances**2)/kernel_width**2))\n",
    "\n",
    "# Print all similarities and weights\n",
    "print(f\"Wasserstein Distances: {similarities_wd}\")\n",
    "print(f\"Cosine Similarities: {similarities_cosine}\")\n",
    "print(f\"Weights: {weights}\")\n",
    "\n",
    "# Optionally print all perturbation texts together for a consolidated view\n",
    "print(\"\\n--- Summary of Perturbations ---\")\n",
    "for i, text in enumerate(perturbation_texts):\n",
    "    print(f\"Perturbation {i + 1}: {text}\")\n",
    "\n",
    "epsilon = 1e-6\n",
    "\n",
    "# Calculate the inverse of each Wasserstein distance, adding a small epsilon to avoid division by zero\n",
    "inverse_similarities_wd = [1.0 / (dist + epsilon) for dist in similarities_wd]\n",
    "\n",
    "# Find the minimum and maximum of the inverse Wasserstein distances\n",
    "min_value = min(inverse_similarities_wd)\n",
    "max_value = max(inverse_similarities_wd)\n",
    "\n",
    "# Scale inverse Wasserstein distances between 0 and 1\n",
    "scaled_similarities_wd = [(value - min_value) / (max_value - min_value) for value in inverse_similarities_wd]\n",
    "Similarities_ = [(value - min_value) / (max_value - min_value) for value in inverse_similarities_wd]\n",
    "print(Similarities_)\n",
    "simpler_model = BayesianRidge()\n",
    "simpler_model.fit(X=perturbations_vect2, y=Similarities_, sample_weight=weights)\n",
    "coeff = simpler_model.coef_\n",
    "# Call the function\n",
    "plot_knowledge_graph_explainability(kg, part_indices, coeff)\n",
    "y_true = np.array(Similarities_).ravel()\n",
    "y_pred = simpler_model.predict(perturbations_vect2).ravel()\n",
    "\n",
    "# Call the function\n",
    "calculate_fidelity_metrics(y_true, y_pred, weights, coeff)\n",
    "# Call the function\n",
    "plot_actual_vs_predicted(y_true, y_pred, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vM5VDCJyAWnC",
   "metadata": {
    "id": "vM5VDCJyAWnC"
   },
   "source": [
    "# Linear : Analyzing Fidelity: Comparing Text and Graph Representations Using Cosine Similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Wv9Ksk_zF0jA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Wv9Ksk_zF0jA",
    "outputId": "46e89edd-9c1b-4331-e039-12cd502f4202"
   },
   "outputs": [],
   "source": [
    "%%timed_and_log_detailed\n",
    "similarities_wd = []\n",
    "similarities_cosine = []\n",
    "perturbations_vect2 = []\n",
    "perturbation_texts = []  # Store the perturbation texts\n",
    "\n",
    "# Loop for perturbations\n",
    "for i in range(20):\n",
    "    # Make a copy of the original vector for each iteration\n",
    "    perturbation_vector = original.copy().flatten()\n",
    "\n",
    "    # Randomly choose one or more parts to remove\n",
    "    num_parts_to_remove = random.randint(1, len(part_names))\n",
    "    parts_to_remove_indices = random.sample(range(len(part_names)), num_parts_to_remove)\n",
    "\n",
    "    # Set the selected parts to 0 in the perturbation vector\n",
    "    for part_idx in parts_to_remove_indices:\n",
    "        perturbation_vector[part_idx] = 0\n",
    "\n",
    "    # Append the perturbation vector to perturbations_vect2\n",
    "    perturbations_vect2.append(perturbation_vector)\n",
    "\n",
    "    # Perturb the KG by removing the selected parts\n",
    "    parts_to_remove = [part_names[idx] for idx in parts_to_remove_indices]\n",
    "    perturbed_kg = perturb_kg_by_removing_parts(kg, parts_to_remove)\n",
    "\n",
    "    # Create a temporary graph for the perturbed KG\n",
    "    graph_temp = NetworkxEntityGraph()\n",
    "    for (node1, relation, node2) in perturbed_kg:\n",
    "        graph_temp.add_triple(KnowledgeTriple(node1, relation, node2))\n",
    "\n",
    "    # Generate response using GraphQAChain\n",
    "    chain = GraphQAChain.from_llm(OpenAI(temperature=0), graph=graph_temp, verbose=False)\n",
    "    temp_response = chain.run(question)\n",
    "\n",
    "    # Store the perturbed response text\n",
    "    perturbation_texts.append(temp_response)\n",
    "\n",
    "    # Get embedding for the perturbed response\n",
    "    temp_response_embedding = get_embedding(temp_response)\n",
    "\n",
    "    # Calculate cosine similarity between the original and perturbed responses\n",
    "    similarity_cosine = 1 - cosine(original_answer_embedding, temp_response_embedding)\n",
    "    similarities_cosine.append(similarity_cosine)\n",
    "\n",
    "    print(f\"Iteration {i + 1}\")\n",
    "    print(f\"Parts removed: {parts_to_remove}\")\n",
    "    print(f\"original_answer response: {original_answer_str}\")\n",
    "    print(f\"Perturbed response: {temp_response}\")\n",
    "    print(f\"Wasserstein Distance with original answer: {similarity_wd}\")\n",
    "    print(f\"Cosine Similarity with original answer: {similarity_cosine}\\n\")\n",
    "\n",
    "# Convert perturbations_vect2 to a numpy array for pairwise distance calculation\n",
    "perturbations_vect2 = np.array(perturbations_vect2)\n",
    "\n",
    "# Calculate cosine distances between perturbation vectors and the original vector\n",
    "distances = sklearn.metrics.pairwise_distances(perturbations_vect2, original, metric='cosine').ravel()\n",
    "\n",
    "# Assuming you may use kernel width in further computations\n",
    "kernel_width = 0.25\n",
    "weights = np.sqrt(np.exp(-(distances**2)/kernel_width**2))\n",
    "\n",
    "# Print all similarities and weights\n",
    "print(f\"Wasserstein Distances: {similarities_wd}\")\n",
    "print(f\"Cosine Similarities: {similarities_cosine}\")\n",
    "print(f\"Weights: {weights}\")\n",
    "\n",
    "# Optionally print all perturbation texts together for a consolidated view\n",
    "print(\"\\n--- Summary of Perturbations ---\")\n",
    "for i, text in enumerate(perturbation_texts):\n",
    "    print(f\"Perturbation {i + 1}: {text}\")\n",
    "simpler_model =  LinearRegression()\n",
    "simpler_model.fit(X=perturbations_vect2, y=similarities_cosine, sample_weight=weights)\n",
    "coeff = simpler_model.coef_\n",
    "# Call the function\n",
    "plot_knowledge_graph_explainability(kg, part_indices, coeff)\n",
    "y_true = np.array(similarities_cosine).ravel()\n",
    "y_pred = simpler_model.predict(perturbations_vect2).ravel()\n",
    "\n",
    "# Call the function\n",
    "calculate_fidelity_metrics(y_true, y_pred, weights, coeff)\n",
    "# Call the function\n",
    "plot_actual_vs_predicted(y_true, y_pred, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b1ea1a",
   "metadata": {
    "id": "a6b1ea1a"
   },
   "source": [
    "# Linear: Fidelity Analysis of Text-to-Text using Inverse Wasserstein Distance and Graph-to-Graph using Cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Qz6ExcRuGIBR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Qz6ExcRuGIBR",
    "outputId": "ebe2560f-356e-47bd-ba3b-4d55851605a8"
   },
   "outputs": [],
   "source": [
    "%%timed_and_log_detailed\n",
    "similarities_wd = []\n",
    "similarities_cosine = []\n",
    "perturbations_vect2 = []\n",
    "perturbation_texts = []  # Store the perturbation texts\n",
    "\n",
    "# Loop for perturbations\n",
    "for i in range(20):\n",
    "    # Make a copy of the original vector for each iteration\n",
    "    perturbation_vector = original.copy().flatten()\n",
    "\n",
    "    # Randomly choose one or more parts to remove\n",
    "    num_parts_to_remove = random.randint(1, len(part_names))\n",
    "    parts_to_remove_indices = random.sample(range(len(part_names)), num_parts_to_remove)\n",
    "\n",
    "    # Set the selected parts to 0 in the perturbation vector\n",
    "    for part_idx in parts_to_remove_indices:\n",
    "        perturbation_vector[part_idx] = 0\n",
    "\n",
    "    # Append the perturbation vector to perturbations_vect2\n",
    "    perturbations_vect2.append(perturbation_vector)\n",
    "\n",
    "    # Perturb the KG by removing the selected parts\n",
    "    parts_to_remove = [part_names[idx] for idx in parts_to_remove_indices]\n",
    "    perturbed_kg = perturb_kg_by_removing_parts(kg, parts_to_remove)\n",
    "\n",
    "    # Create a temporary graph for the perturbed KG\n",
    "    graph_temp = NetworkxEntityGraph()\n",
    "    for (node1, relation, node2) in perturbed_kg:\n",
    "        graph_temp.add_triple(KnowledgeTriple(node1, relation, node2))\n",
    "\n",
    "    # Generate response using GraphQAChain\n",
    "    chain = GraphQAChain.from_llm(OpenAI(temperature=0), graph=graph_temp, verbose=False)\n",
    "    temp_response = chain.run(question)\n",
    "\n",
    "    # Store the perturbed response text\n",
    "    perturbation_texts.append(temp_response)\n",
    "\n",
    "    # Get embedding for the perturbed response\n",
    "    temp_response_embedding = get_embedding(temp_response)\n",
    "\n",
    "\n",
    "    # Calculate Wasserstein distance between the original and perturbed responses\n",
    "    similarity_wd = wasserstein_distance(original_answer_embedding, temp_response_embedding)\n",
    "    similarities_wd.append(similarity_wd)\n",
    "\n",
    "    # Calculate cosine similarity between the original and perturbed responses\n",
    "    similarity_cosine = 1 - cosine(original_answer_embedding, temp_response_embedding)\n",
    "    similarities_cosine.append(similarity_cosine)\n",
    "\n",
    "    print(f\"Iteration {i + 1}\")\n",
    "    print(f\"Parts removed: {parts_to_remove}\")\n",
    "    print(f\"original_answer response: {original_answer_str}\")\n",
    "    print(f\"Perturbed response: {temp_response}\")\n",
    "    print(f\"Wasserstein Distance with original answer: {similarity_wd}\")\n",
    "    print(f\"Cosine Similarity with original answer: {similarity_cosine}\\n\")\n",
    "\n",
    "# Convert perturbations_vect2 to a numpy array for pairwise distance calculation\n",
    "perturbations_vect2 = np.array(perturbations_vect2)\n",
    "\n",
    "# Calculate cosine distances between perturbation vectors and the original vector\n",
    "distances = sklearn.metrics.pairwise_distances(perturbations_vect2, original, metric='cosine').ravel()\n",
    "\n",
    "# Assuming you may use kernel width in further computations\n",
    "kernel_width = 0.25\n",
    "weights = np.sqrt(np.exp(-(distances**2)/kernel_width**2))\n",
    "\n",
    "# Print all similarities and weights\n",
    "print(f\"Wasserstein Distances: {similarities_wd}\")\n",
    "print(f\"Cosine Similarities: {similarities_cosine}\")\n",
    "print(f\"Weights: {weights}\")\n",
    "\n",
    "# Optionally print all perturbation texts together for a consolidated view\n",
    "print(\"\\n--- Summary of Perturbations ---\")\n",
    "for i, text in enumerate(perturbation_texts):\n",
    "    print(f\"Perturbation {i + 1}: {text}\")\n",
    "\n",
    "epsilon = 1e-6\n",
    "\n",
    "# Calculate the inverse of each Wasserstein distance, adding a small epsilon to avoid division by zero\n",
    "inverse_similarities_wd = [1.0 / (dist + epsilon) for dist in similarities_wd]\n",
    "\n",
    "# Find the minimum and maximum of the inverse Wasserstein distances\n",
    "min_value = min(inverse_similarities_wd)\n",
    "max_value = max(inverse_similarities_wd)\n",
    "\n",
    "# Scale inverse Wasserstein distances between 0 and 1\n",
    "scaled_similarities_wd = [(value - min_value) / (max_value - min_value) for value in inverse_similarities_wd]\n",
    "Similarities_ = [(value - min_value) / (max_value - min_value) for value in inverse_similarities_wd]\n",
    "print(Similarities_)\n",
    "simpler_model =  LinearRegression()\n",
    "simpler_model.fit(X=perturbations_vect2, y=Similarities_, sample_weight=weights)\n",
    "coeff = simpler_model.coef_\n",
    "# Call the function\n",
    "plot_knowledge_graph_explainability(kg, part_indices, coeff)\n",
    "y_true = np.array(Similarities_).ravel()\n",
    "y_pred = simpler_model.predict(perturbations_vect2).ravel()\n",
    "\n",
    "# Call the function\n",
    "calculate_fidelity_metrics(y_true, y_pred, weights, coeff)\n",
    "# Call the function\n",
    "plot_actual_vs_predicted(y_true, y_pred, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc64b88c",
   "metadata": {
    "id": "cc64b88c"
   },
   "source": [
    "# Linear : Dual Metrics: Blending Inverse Wasserstein Distance and Cosine Similarity for Text and Graph-to-Graph using Cosine\n",
    "Cosine Similarity: Measuring Alignment and Fidelity Between Textual Representations\n",
    "Wasserstein Distance: Evaluating Semantic Shifts Between Textual Representations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SBxWSADGHI7_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "SBxWSADGHI7_",
    "outputId": "37054cce-120b-436d-a27e-b810378d8ac3"
   },
   "outputs": [],
   "source": [
    "%%timed_and_log_detailed\n",
    "similarities_wd = []\n",
    "similarities_cosine = []\n",
    "perturbations_vect2 = []\n",
    "perturbation_texts = []  # Store the perturbation texts\n",
    "\n",
    "# Loop for perturbations\n",
    "for i in range(20):\n",
    "    # Make a copy of the original vector for each iteration\n",
    "    perturbation_vector = original.copy().flatten()\n",
    "\n",
    "    # Randomly choose one or more parts to remove\n",
    "    num_parts_to_remove = random.randint(1, len(part_names))\n",
    "    parts_to_remove_indices = random.sample(range(len(part_names)), num_parts_to_remove)\n",
    "\n",
    "    # Set the selected parts to 0 in the perturbation vector\n",
    "    for part_idx in parts_to_remove_indices:\n",
    "        perturbation_vector[part_idx] = 0\n",
    "\n",
    "    # Append the perturbation vector to perturbations_vect2\n",
    "    perturbations_vect2.append(perturbation_vector)\n",
    "\n",
    "    # Perturb the KG by removing the selected parts\n",
    "    parts_to_remove = [part_names[idx] for idx in parts_to_remove_indices]\n",
    "    perturbed_kg = perturb_kg_by_removing_parts(kg, parts_to_remove)\n",
    "\n",
    "    # Create a temporary graph for the perturbed KG\n",
    "    graph_temp = NetworkxEntityGraph()\n",
    "    for (node1, relation, node2) in perturbed_kg:\n",
    "        graph_temp.add_triple(KnowledgeTriple(node1, relation, node2))\n",
    "\n",
    "    # Generate response using GraphQAChain\n",
    "    chain = GraphQAChain.from_llm(OpenAI(temperature=0), graph=graph_temp, verbose=False)\n",
    "    temp_response = chain.run(question)\n",
    "\n",
    "    # Store the perturbed response text\n",
    "    perturbation_texts.append(temp_response)\n",
    "\n",
    "    # Get embedding for the perturbed response\n",
    "    temp_response_embedding = get_embedding(temp_response)\n",
    "\n",
    "\n",
    "    # Calculate Wasserstein distance between the original and perturbed responses\n",
    "    similarity_wd = wasserstein_distance(original_answer_embedding, temp_response_embedding)\n",
    "    similarities_wd.append(similarity_wd)\n",
    "\n",
    "    # Calculate cosine similarity between the original and perturbed responses\n",
    "    similarity_cosine = 1 - cosine(original_answer_embedding, temp_response_embedding)\n",
    "    similarities_cosine.append(similarity_cosine)\n",
    "\n",
    "    print(f\"Iteration {i + 1}\")\n",
    "    print(f\"Parts removed: {parts_to_remove}\")\n",
    "    print(f\"original_answer response: {original_answer_str}\")\n",
    "    print(f\"Perturbed response: {temp_response}\")\n",
    "    print(f\"Wasserstein Distance with original answer: {similarity_wd}\")\n",
    "    print(f\"Cosine Similarity with original answer: {similarity_cosine}\\n\")\n",
    "\n",
    "# Convert perturbations_vect2 to a numpy array for pairwise distance calculation\n",
    "perturbations_vect2 = np.array(perturbations_vect2)\n",
    "\n",
    "# Calculate cosine distances between perturbation vectors and the original vector\n",
    "distances = sklearn.metrics.pairwise_distances(perturbations_vect2, original, metric='cosine').ravel()\n",
    "\n",
    "# Assuming you may use kernel width in further computations\n",
    "kernel_width = 0.25\n",
    "weights = np.sqrt(np.exp(-(distances**2)/kernel_width**2))\n",
    "\n",
    "# Print all similarities and weights\n",
    "print(f\"Wasserstein Distances: {similarities_wd}\")\n",
    "print(f\"Cosine Similarities: {similarities_cosine}\")\n",
    "print(f\"Weights: {weights}\")\n",
    "\n",
    "# Optionally print all perturbation texts together for a consolidated view\n",
    "print(\"\\n--- Summary of Perturbations ---\")\n",
    "for i, text in enumerate(perturbation_texts):\n",
    "    print(f\"Perturbation {i + 1}: {text}\")#bayLime_sum_inv_wd_cosine\n",
    "\n",
    "\n",
    "#bayLime_sum_inv_wd_cosine\n",
    "epsilon = 1e-6\n",
    "\n",
    "# Calculate the inverse of each Wasserstein distance, adding a small epsilon to avoid division by zero\n",
    "inverse_similarities_wd = [1.0 / (dist + epsilon) for dist in similarities_wd]\n",
    "\n",
    "# Find the minimum and maximum of the inverse Wasserstein distances\n",
    "min_value = min(inverse_similarities_wd)\n",
    "max_value = max(inverse_similarities_wd)\n",
    "\n",
    "# Scale inverse Wasserstein distances between 0 and 1\n",
    "scaled_similarities_wd = [(value - min_value) / (max_value - min_value) for value in inverse_similarities_wd]\n",
    "\n",
    "# Combine the scaled inverse Wasserstein distances and cosine similarities\n",
    "Similarities_ = [wd + cos for wd, cos in zip(scaled_similarities_wd, similarities_cosine)]\n",
    "# Print the combined list of similarities\n",
    "print(Similarities_)\n",
    "\n",
    "simpler_model = LinearRegression()\n",
    "simpler_model.fit(X=perturbations_vect2, y=Similarities_, sample_weight=weights)\n",
    "coeff = simpler_model.coef_\n",
    "# Call the function\n",
    "plot_knowledge_graph_explainability(kg, part_indices, coeff)\n",
    "y_true = np.array(Similarities_).ravel()\n",
    "y_pred = simpler_model.predict(perturbations_vect2).ravel()\n",
    "\n",
    "# Call the function\n",
    "calculate_fidelity_metrics(y_true, y_pred, weights, coeff)\n",
    "# Call the function\n",
    "plot_actual_vs_predicted(y_true, y_pred, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040e8098",
   "metadata": {
    "id": "040e8098"
   },
   "source": [
    "# Linear: Fidelity Analysis of Text-to-Text using Wasserstein Distance and Graph-to-Graph using Cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "J_spTmYGGg5J",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "J_spTmYGGg5J",
    "outputId": "0fee2d40-9e28-4b8b-a517-01a404d10c48"
   },
   "outputs": [],
   "source": [
    "%%timed_and_log_detailed\n",
    "similarities_wd = []\n",
    "similarities_cosine = []\n",
    "perturbations_vect2 = []\n",
    "perturbation_texts = []  # Store the perturbation texts\n",
    "\n",
    "# Loop for perturbations\n",
    "for i in range(20):\n",
    "    # Make a copy of the original vector for each iteration\n",
    "    perturbation_vector = original.copy().flatten()\n",
    "\n",
    "    # Randomly choose one or more parts to remove\n",
    "    num_parts_to_remove = random.randint(1, len(part_names))\n",
    "    parts_to_remove_indices = random.sample(range(len(part_names)), num_parts_to_remove)\n",
    "\n",
    "    # Set the selected parts to 0 in the perturbation vector\n",
    "    for part_idx in parts_to_remove_indices:\n",
    "        perturbation_vector[part_idx] = 0\n",
    "\n",
    "    # Append the perturbation vector to perturbations_vect2\n",
    "    perturbations_vect2.append(perturbation_vector)\n",
    "\n",
    "    # Perturb the KG by removing the selected parts\n",
    "    parts_to_remove = [part_names[idx] for idx in parts_to_remove_indices]\n",
    "    perturbed_kg = perturb_kg_by_removing_parts(kg, parts_to_remove)\n",
    "\n",
    "    # Create a temporary graph for the perturbed KG\n",
    "    graph_temp = NetworkxEntityGraph()\n",
    "    for (node1, relation, node2) in perturbed_kg:\n",
    "        graph_temp.add_triple(KnowledgeTriple(node1, relation, node2))\n",
    "\n",
    "    # Generate response using GraphQAChain\n",
    "    chain = GraphQAChain.from_llm(OpenAI(temperature=0), graph=graph_temp, verbose=False)\n",
    "    temp_response = chain.run(question)\n",
    "\n",
    "    # Store the perturbed response text\n",
    "    perturbation_texts.append(temp_response)\n",
    "\n",
    "    # Get embedding for the perturbed response\n",
    "    temp_response_embedding = get_embedding(temp_response)\n",
    "\n",
    "\n",
    "    # Calculate Wasserstein distance between the original and perturbed responses\n",
    "    similarity_wd = wasserstein_distance(original_answer_embedding, temp_response_embedding)\n",
    "    similarities_wd.append(similarity_wd)\n",
    "\n",
    "    # Calculate cosine similarity between the original and perturbed responses\n",
    "    similarity_cosine = 1 - cosine(original_answer_embedding, temp_response_embedding)\n",
    "    similarities_cosine.append(similarity_cosine)\n",
    "\n",
    "    print(f\"Iteration {i + 1}\")\n",
    "    print(f\"Parts removed: {parts_to_remove}\")\n",
    "    print(f\"original_answer response: {original_answer_str}\")\n",
    "    print(f\"Perturbed response: {temp_response}\")\n",
    "    print(f\"Wasserstein Distance with original answer: {similarity_wd}\")\n",
    "    print(f\"Cosine Similarity with original answer: {similarity_cosine}\\n\")\n",
    "\n",
    "# Convert perturbations_vect2 to a numpy array for pairwise distance calculation\n",
    "perturbations_vect2 = np.array(perturbations_vect2)\n",
    "\n",
    "# Calculate cosine distances between perturbation vectors and the original vector\n",
    "distances = sklearn.metrics.pairwise_distances(perturbations_vect2, original, metric='cosine').ravel()\n",
    "\n",
    "# Assuming you may use kernel width in further computations\n",
    "kernel_width = 0.25\n",
    "weights = np.sqrt(np.exp(-(distances**2)/kernel_width**2))\n",
    "\n",
    "# Print all similarities and weights\n",
    "print(f\"Wasserstein Distances: {similarities_wd}\")\n",
    "print(f\"Cosine Similarities: {similarities_cosine}\")\n",
    "print(f\"Weights: {weights}\")\n",
    "\n",
    "# Optionally print all perturbation texts together for a consolidated view\n",
    "print(\"\\n--- Summary of Perturbations ---\")\n",
    "for i, text in enumerate(perturbation_texts):\n",
    "    print(f\"Perturbation {i + 1}: {text}\")#bayLime_sum_inv_wd_cosine\n",
    "\n",
    "min_value_wd= min(similarities_wd)\n",
    "max_value_wd = max(similarities_wd)\n",
    "# Scale between 0 and 1\n",
    "Similarities_ = [(value - min_value_wd) / (max_value_wd - min_value_wd) for value in similarities_wd]\n",
    "print(Similarities_)\n",
    "\n",
    "simpler_model = LinearRegression()\n",
    "simpler_model.fit(X=perturbations_vect2, y= Similarities_, sample_weight=weights)\n",
    "coeff = simpler_model.coef_\n",
    "# Call the function\n",
    "plot_knowledge_graph_explainability(kg, part_indices, coeff)\n",
    "y_true = np.array(Similarities_).ravel()\n",
    "y_pred = simpler_model.predict(perturbations_vect2).ravel()\n",
    "\n",
    "# Call the function\n",
    "calculate_fidelity_metrics(y_true, y_pred, weights, coeff)\n",
    "# Call the function\n",
    "plot_actual_vs_predicted(y_true, y_pred, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59def70",
   "metadata": {
    "id": "d59def70"
   },
   "source": [
    "# BayLime: Fidelity Analysis of Text-to-Text using Wasserstein Distance and Graph-to-Graph using Cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RQAh7YSPITPm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "RQAh7YSPITPm",
    "outputId": "21884173-41a0-487b-a3d0-75214d8628f9"
   },
   "outputs": [],
   "source": [
    "%%timed_and_log_detailed\n",
    "similarities_wd = []\n",
    "similarities_cosine = []\n",
    "perturbations_vect2 = []\n",
    "perturbation_texts = []  # Store the perturbation texts\n",
    "\n",
    "# Loop for perturbations\n",
    "for i in range(20):\n",
    "    # Make a copy of the original vector for each iteration\n",
    "    perturbation_vector = original.copy().flatten()\n",
    "\n",
    "    # Randomly choose one or more parts to remove\n",
    "    num_parts_to_remove = random.randint(1, len(part_names))\n",
    "    parts_to_remove_indices = random.sample(range(len(part_names)), num_parts_to_remove)\n",
    "\n",
    "    # Set the selected parts to 0 in the perturbation vector\n",
    "    for part_idx in parts_to_remove_indices:\n",
    "        perturbation_vector[part_idx] = 0\n",
    "\n",
    "    # Append the perturbation vector to perturbations_vect2\n",
    "    perturbations_vect2.append(perturbation_vector)\n",
    "\n",
    "    # Perturb the KG by removing the selected parts\n",
    "    parts_to_remove = [part_names[idx] for idx in parts_to_remove_indices]\n",
    "    perturbed_kg = perturb_kg_by_removing_parts(kg, parts_to_remove)\n",
    "\n",
    "    # Create a temporary graph for the perturbed KG\n",
    "    graph_temp = NetworkxEntityGraph()\n",
    "    for (node1, relation, node2) in perturbed_kg:\n",
    "        graph_temp.add_triple(KnowledgeTriple(node1, relation, node2))\n",
    "\n",
    "    # Generate response using GraphQAChain\n",
    "    chain = GraphQAChain.from_llm(OpenAI(temperature=0), graph=graph_temp, verbose=False)\n",
    "    temp_response = chain.run(question)\n",
    "\n",
    "    # Store the perturbed response text\n",
    "    perturbation_texts.append(temp_response)\n",
    "\n",
    "    # Get embedding for the perturbed response\n",
    "    temp_response_embedding = get_embedding(temp_response)\n",
    "\n",
    "\n",
    "    # Calculate Wasserstein distance between the original and perturbed responses\n",
    "    similarity_wd = wasserstein_distance(original_answer_embedding, temp_response_embedding)\n",
    "    similarities_wd.append(similarity_wd)\n",
    "\n",
    "    # Calculate cosine similarity between the original and perturbed responses\n",
    "    similarity_cosine = 1 - cosine(original_answer_embedding, temp_response_embedding)\n",
    "    similarities_cosine.append(similarity_cosine)\n",
    "\n",
    "    print(f\"Iteration {i + 1}\")\n",
    "    print(f\"Parts removed: {parts_to_remove}\")\n",
    "    print(f\"original_answer response: {original_answer_str}\")\n",
    "    print(f\"Perturbed response: {temp_response}\")\n",
    "    print(f\"Wasserstein Distance with original answer: {similarity_wd}\")\n",
    "    print(f\"Cosine Similarity with original answer: {similarity_cosine}\\n\")\n",
    "\n",
    "# Convert perturbations_vect2 to a numpy array for pairwise distance calculation\n",
    "perturbations_vect2 = np.array(perturbations_vect2)\n",
    "\n",
    "# Calculate cosine distances between perturbation vectors and the original vector\n",
    "distances = sklearn.metrics.pairwise_distances(perturbations_vect2, original, metric='cosine').ravel()\n",
    "\n",
    "# Assuming you may use kernel width in further computations\n",
    "kernel_width = 0.25\n",
    "weights = np.sqrt(np.exp(-(distances**2)/kernel_width**2))\n",
    "\n",
    "# Print all similarities and weights\n",
    "print(f\"Wasserstein Distances: {similarities_wd}\")\n",
    "print(f\"Cosine Similarities: {similarities_cosine}\")\n",
    "print(f\"Weights: {weights}\")\n",
    "\n",
    "# Optionally print all perturbation texts together for a consolidated view\n",
    "print(\"\\n--- Summary of Perturbations ---\")\n",
    "for i, text in enumerate(perturbation_texts):\n",
    "    print(f\"Perturbation {i + 1}: {text}\")#bayLime_sum_inv_wd_cosine\n",
    "\n",
    "min_value_wd= min(similarities_wd)\n",
    "max_value_wd = max(similarities_wd)\n",
    "# Scale between 0 and 1\n",
    "Similarities_ = [(value - min_value_wd) / (max_value_wd - min_value_wd) for value in similarities_wd]\n",
    "print(Similarities_)\n",
    "\n",
    "simpler_model =  BayesianRidge()\n",
    "simpler_model.fit(X=perturbations_vect2, y= Similarities_, sample_weight=weights)\n",
    "coeff = simpler_model.coef_\n",
    "# Call the function\n",
    "plot_knowledge_graph_explainability(kg, part_indices, coeff)\n",
    "y_true = np.array(Similarities_).ravel()\n",
    "y_pred = simpler_model.predict(perturbations_vect2).ravel()\n",
    "\n",
    "# Call the function\n",
    "calculate_fidelity_metrics(y_true, y_pred, weights, coeff)\n",
    "# Call the function\n",
    "plot_actual_vs_predicted(y_true, y_pred, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab2b401",
   "metadata": {
    "id": "cab2b401"
   },
   "source": [
    "# BayLIME: Hybrid Text Metrics (Wasserstein Distance + Cosine) vs. Graph Metrics (Cosine) Fidelity Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7843cd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "8e7843cd",
    "outputId": "a9ffecfb-1e37-4ce2-fd88-b7cbc806613f"
   },
   "outputs": [],
   "source": [
    "%%timed_and_log_detailed\n",
    "similarities_wd = []\n",
    "similarities_cosine = []\n",
    "perturbations_vect2 = []\n",
    "perturbation_texts = []  # Store the perturbation texts\n",
    "\n",
    "# Loop for perturbations\n",
    "for i in range(20):\n",
    "    # Make a copy of the original vector for each iteration\n",
    "    perturbation_vector = original.copy().flatten()\n",
    "\n",
    "    # Randomly choose one or more parts to remove\n",
    "    num_parts_to_remove = random.randint(1, len(part_names))\n",
    "    parts_to_remove_indices = random.sample(range(len(part_names)), num_parts_to_remove)\n",
    "\n",
    "    # Set the selected parts to 0 in the perturbation vector\n",
    "    for part_idx in parts_to_remove_indices:\n",
    "        perturbation_vector[part_idx] = 0\n",
    "\n",
    "    # Append the perturbation vector to perturbations_vect2\n",
    "    perturbations_vect2.append(perturbation_vector)\n",
    "\n",
    "    # Perturb the KG by removing the selected parts\n",
    "    parts_to_remove = [part_names[idx] for idx in parts_to_remove_indices]\n",
    "    perturbed_kg = perturb_kg_by_removing_parts(kg, parts_to_remove)\n",
    "\n",
    "    # Create a temporary graph for the perturbed KG\n",
    "    graph_temp = NetworkxEntityGraph()\n",
    "    for (node1, relation, node2) in perturbed_kg:\n",
    "        graph_temp.add_triple(KnowledgeTriple(node1, relation, node2))\n",
    "\n",
    "    # Generate response using GraphQAChain\n",
    "    chain = GraphQAChain.from_llm(OpenAI(temperature=0), graph=graph_temp, verbose=False)\n",
    "    temp_response = chain.run(question)\n",
    "\n",
    "    # Store the perturbed response text\n",
    "    perturbation_texts.append(temp_response)\n",
    "\n",
    "    # Get embedding for the perturbed response\n",
    "    temp_response_embedding = get_embedding(temp_response)\n",
    "\n",
    "\n",
    "    # Calculate Wasserstein distance between the original and perturbed responses\n",
    "    similarity_wd = wasserstein_distance(original_answer_embedding, temp_response_embedding)\n",
    "    similarities_wd.append(similarity_wd)\n",
    "\n",
    "    # Calculate cosine similarity between the original and perturbed responses\n",
    "    similarity_cosine = 1 - cosine(original_answer_embedding, temp_response_embedding)\n",
    "    similarities_cosine.append(similarity_cosine)\n",
    "\n",
    "    print(f\"Iteration {i + 1}\")\n",
    "    print(f\"Parts removed: {parts_to_remove}\")\n",
    "    print(f\"original_answer response: {original_answer_str}\")\n",
    "    print(f\"Perturbed response: {temp_response}\")\n",
    "    print(f\"Wasserstein Distance with original answer: {similarity_wd}\")\n",
    "    print(f\"Cosine Similarity with original answer: {similarity_cosine}\\n\")\n",
    "\n",
    "# Convert perturbations_vect2 to a numpy array for pairwise distance calculation\n",
    "perturbations_vect2 = np.array(perturbations_vect2)\n",
    "\n",
    "# Calculate cosine distances between perturbation vectors and the original vector\n",
    "distances = sklearn.metrics.pairwise_distances(perturbations_vect2, original, metric='cosine').ravel()\n",
    "\n",
    "# Assuming you may use kernel width in further computations\n",
    "kernel_width = 0.25\n",
    "weights = np.sqrt(np.exp(-(distances**2)/kernel_width**2))\n",
    "\n",
    "# Print all similarities and weights\n",
    "print(f\"Wasserstein Distances: {similarities_wd}\")\n",
    "print(f\"Cosine Similarities: {similarities_cosine}\")\n",
    "print(f\"Weights: {weights}\")\n",
    "\n",
    "# Optionally print all perturbation texts together for a consolidated view\n",
    "print(\"\\n--- Summary of Perturbations ---\")\n",
    "for i, text in enumerate(perturbation_texts):\n",
    "    print(f\"Perturbation {i + 1}: {text}\")\n",
    "# Combine the scaled inverse Wasserstein distances and cosine similarities\n",
    "Similarities= [wd + cos for wd, cos in zip(similarities_wd, similarities_cosine)]\n",
    "# Print the combined list of similarities\n",
    "print(Similarities)\n",
    "simpler_model = BayesianRidge()\n",
    "simpler_model.fit(X=perturbations_vect2, y=Similarities, sample_weight=weights)\n",
    "coeff = simpler_model.coef_\n",
    "# Call the function\n",
    "plot_knowledge_graph_explainability(kg, part_indices, coeff)\n",
    "y_true = np.array(Similarities).ravel()\n",
    "y_pred = simpler_model.predict(perturbations_vect2).ravel()\n",
    "\n",
    "# Call the function\n",
    "calculate_fidelity_metrics(y_true, y_pred, weights, coeff)\n",
    "# Call the function\n",
    "plot_actual_vs_predicted(y_true, y_pred, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146b2198",
   "metadata": {
    "id": "146b2198"
   },
   "source": [
    "# Linear: Hybrid Text Metrics (Wasserstein Distance + Cosine) vs. Graph Metrics (Cosine) Fidelity Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "K_idUJjLI1-6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "K_idUJjLI1-6",
    "outputId": "ccca0993-aa77-4c49-dab5-c74b75866e2b"
   },
   "outputs": [],
   "source": [
    "%%timed_and_log_detailed\n",
    "similarities_wd = []\n",
    "similarities_cosine = []\n",
    "perturbations_vect2 = []\n",
    "perturbation_texts = []  # Store the perturbation texts\n",
    "\n",
    "# Loop for perturbations\n",
    "for i in range(20):\n",
    "    # Make a copy of the original vector for each iteration\n",
    "    perturbation_vector = original.copy().flatten()\n",
    "\n",
    "    # Randomly choose one or more parts to remove\n",
    "    num_parts_to_remove = random.randint(1, len(part_names))\n",
    "    parts_to_remove_indices = random.sample(range(len(part_names)), num_parts_to_remove)\n",
    "\n",
    "    # Set the selected parts to 0 in the perturbation vector\n",
    "    for part_idx in parts_to_remove_indices:\n",
    "        perturbation_vector[part_idx] = 0\n",
    "\n",
    "    # Append the perturbation vector to perturbations_vect2\n",
    "    perturbations_vect2.append(perturbation_vector)\n",
    "\n",
    "    # Perturb the KG by removing the selected parts\n",
    "    parts_to_remove = [part_names[idx] for idx in parts_to_remove_indices]\n",
    "    perturbed_kg = perturb_kg_by_removing_parts(kg, parts_to_remove)\n",
    "\n",
    "    # Create a temporary graph for the perturbed KG\n",
    "    graph_temp = NetworkxEntityGraph()\n",
    "    for (node1, relation, node2) in perturbed_kg:\n",
    "        graph_temp.add_triple(KnowledgeTriple(node1, relation, node2))\n",
    "\n",
    "    # Generate response using GraphQAChain\n",
    "    chain = GraphQAChain.from_llm(OpenAI(temperature=0), graph=graph_temp, verbose=False)\n",
    "    temp_response = chain.run(question)\n",
    "\n",
    "    # Store the perturbed response text\n",
    "    perturbation_texts.append(temp_response)\n",
    "\n",
    "    # Get embedding for the perturbed response\n",
    "    temp_response_embedding = get_embedding(temp_response)\n",
    "\n",
    "\n",
    "    # Calculate Wasserstein distance between the original and perturbed responses\n",
    "    similarity_wd = wasserstein_distance(original_answer_embedding, temp_response_embedding)\n",
    "    similarities_wd.append(similarity_wd)\n",
    "\n",
    "    # Calculate cosine similarity between the original and perturbed responses\n",
    "    similarity_cosine = 1 - cosine(original_answer_embedding, temp_response_embedding)\n",
    "    similarities_cosine.append(similarity_cosine)\n",
    "\n",
    "    print(f\"Iteration {i + 1}\")\n",
    "    print(f\"Parts removed: {parts_to_remove}\")\n",
    "    print(f\"original_answer response: {original_answer_str}\")\n",
    "    print(f\"Perturbed response: {temp_response}\")\n",
    "    print(f\"Wasserstein Distance with original answer: {similarity_wd}\")\n",
    "    print(f\"Cosine Similarity with original answer: {similarity_cosine}\\n\")\n",
    "\n",
    "# Convert perturbations_vect2 to a numpy array for pairwise distance calculation\n",
    "perturbations_vect2 = np.array(perturbations_vect2)\n",
    "\n",
    "# Calculate cosine distances between perturbation vectors and the original vector\n",
    "distances = sklearn.metrics.pairwise_distances(perturbations_vect2, original, metric='cosine').ravel()\n",
    "\n",
    "# Assuming you may use kernel width in further computations\n",
    "kernel_width = 0.25\n",
    "weights = np.sqrt(np.exp(-(distances**2)/kernel_width**2))\n",
    "\n",
    "# Print all similarities and weights\n",
    "print(f\"Wasserstein Distances: {similarities_wd}\")\n",
    "print(f\"Cosine Similarities: {similarities_cosine}\")\n",
    "print(f\"Weights: {weights}\")\n",
    "\n",
    "# Optionally print all perturbation texts together for a consolidated view\n",
    "print(\"\\n--- Summary of Perturbations ---\")\n",
    "for i, text in enumerate(perturbation_texts):\n",
    "    print(f\"Perturbation {i + 1}: {text}\")\n",
    "# Combine the scaled inverse Wasserstein distances and cosine similarities\n",
    "Similarities= [wd + cos for wd, cos in zip(similarities_wd, similarities_cosine)]\n",
    "# Print the combined list of similarities\n",
    "print(Similarities)\n",
    "simpler_model = LinearRegression()\n",
    "simpler_model.fit(X=perturbations_vect2, y=Similarities, sample_weight=weights)\n",
    "coeff = simpler_model.coef_\n",
    "# Call the function\n",
    "plot_knowledge_graph_explainability(kg, part_indices, coeff)\n",
    "y_true = np.array(Similarities).ravel()\n",
    "y_pred = simpler_model.predict(perturbations_vect2).ravel()\n",
    "\n",
    "# Call the function\n",
    "calculate_fidelity_metrics(y_true, y_pred, weights, coeff)\n",
    "# Call the function\n",
    "plot_actual_vs_predicted(y_true, y_pred, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962fbecd-74a8-454b-8c8d-419a7bbba97d",
   "metadata": {
    "id": "962fbecd-74a8-454b-8c8d-419a7bbba97d"
   },
   "source": [
    "# Time Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bf6ad6-0149-47e2-893c-969a1f3fd6f6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f4bf6ad6-0149-47e2-893c-969a1f3fd6f6",
    "outputId": "7ad8fea4-440d-495c-ad5f-d7833c83f67b"
   },
   "outputs": [],
   "source": [
    "# Define metric names corresponding to each run\n",
    "metric_names = [\"BayLIME: Dual Metrics: Blending Inverse Wasserstein Distance and Cosine Similarity for Text and Graph-to-Graph using Cosine\",\n",
    "                \"BayLIME: Analyzing Fidelity: Comparing Text and Graph Representations Using Cosine Similarity\",\n",
    "                \"BayLIME: Fidelity Analysis of Text-to-Text using Inverse Wasserstein Distance and Graph-to-Graph using Cosine\",\n",
    "                \"Linear : Analyzing Fidelity: Comparing Text and Graph Representations Using Cosine Similarity\",\n",
    "                \"Linear: Fidelity Analysis of Text-to-Text using Inverse Wasserstein Distance and Graph-to-Graph using Cosine\",\n",
    "                \"Linear : Dual Metrics: Blending Inverse Wasserstein Distance and Cosine Similarity for Text and Graph-to-Graph using Cosine\",\n",
    "                \"Linear: Fidelity Analysis of Text-to-Text using Wasserstein Distance and Graph-to-Graph using Cosine\",\n",
    "                \"BayLime: Fidelity Analysis of Text-to-Text using Wasserstein Distance and Graph-to-Graph using Cosine\",\n",
    "                \"BayLIME: Hybrid Text Metrics (Wasserstein Distance + Cosine) vs. Graph Metrics (Cosine) Fidelity Analysis\",\n",
    "                \"Linear: Hybrid Text Metrics (Wasserstein Distance + Cosine) vs. Graph Metrics (Cosine) Fidelity Analysis\"\n",
    "                ]\n",
    "\n",
    "# Print all logged runtimes with metric names\n",
    "for i, (runtime, metric) in enumerate(zip(runtimes, metric_names), start=1):\n",
    "    print(f\"Metric: {metric}\")\n",
    "    print(f\"  CPU times: user {runtime['user_time']:.2f} s, sys {runtime['sys_time']:.2f} s, total {runtime['total_cpu_time']:.2f} s\")\n",
    "    print(f\"  Wall time: {runtime['wall_time']:.2f} s\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iP--A9Y7dm72",
   "metadata": {
    "id": "iP--A9Y7dm72"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521c7907-8fb9-4278-b013-58ede3988c60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "040e8098",
    "d59def70"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
